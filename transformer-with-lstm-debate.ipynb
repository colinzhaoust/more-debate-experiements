{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "565efe44ca706182eca35d63c14eda622e966b43"
   },
   "source": [
    "Some thoughts:\n",
    "\n",
    "* Is attention all you need? That may be true for seq2seq learning, but not in text summarization\n",
    "\n",
    "* Since other Kagglers found adding attention to LSTM helps, why not add the multi head attention to LSTM?\n",
    "\n",
    "* Blending rocks (multi head attention)\n",
    "\n",
    "\n",
    "Reference:\n",
    "\n",
    "* Attention Is All You Need: https://arxiv.org/abs/1706.03762\n",
    "\n",
    "* Transformer in Keras: https://github.com/Lsdefine/attention-is-all-you-need-keras/blob/master/transformer.py\n",
    "    \n",
    "* SRK: https://www.kaggle.com/sudalairajkumar/a-look-at-different-embeddings . There is not much changed from this kernel except the nueral net architecture and final weights of the embeddings.\n",
    "\n",
    "* Attention model from Khoi Ngyuen: https://www.kaggle.com/suicaokhoailang/lstm-attention-baseline-0-652-lb\n",
    "\n",
    "* https://www.kaggle.com/shujian/different-embeddings-with-attention-fork-fork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "# print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, Flatten, GlobalAveragePooling1D, Reshape\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "7612d1a3d79754ca6a48cc21d42391b1917255bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw data shape:  (34740, 50)\n"
     ]
    }
   ],
   "source": [
    "# train_df = pd.read_csv(\"train.csv\")\n",
    "# val_df = pd.read_csv(\"valid.csv\")\n",
    "# test_df = pd.read_csv(\"test.csv\")\n",
    "# print(\"Train shape : \",train_df.shape)\n",
    "# print(\"Valid shape : \",val_df.shape)\n",
    "# print(\"Test shape : \",test_df.shape)\n",
    "\n",
    "s_df = pd.read_csv(\"allsides.tsv\",sep='\\t')\n",
    "df = pd.read_csv(\"debate_text.tsv\",sep='\\t')\n",
    "\n",
    "print(\"raw data shape: \", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "72530a51393c9d3697b5127be319aff3ddd860f7"
   },
   "outputs": [],
   "source": [
    "# Let's start with debt\n",
    "target_df = df[:10000].dropna()\n",
    "target = \"Abortion\"\n",
    "\n",
    "target_labels = target_df[target].values\n",
    "index_to_remove = np.where(target_labels > 1)\n",
    "target_df = target_df.drop(index_to_remove[0])\n",
    "\n",
    "\n",
    "## split to train and val\n",
    "train_df, val_df = train_test_split(target_df, test_size=0.1, random_state=2019)\n",
    "pretrain_df = s_df.dropna()\n",
    "\n",
    "\n",
    "## some config values \n",
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 68000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 1500 # max number of words in a question to use\n",
    "\n",
    "\n",
    "## fill up the missing values\n",
    "pretrain_X = pretrain_df[\"text\"].fillna(\"_##_\").values\n",
    "train_X = train_df[\"text\"].fillna(\"_##_\").values\n",
    "val_X = val_df[\"text\"].fillna(\"_##_\").values\n",
    "\n",
    "\n",
    "## Tokenize the sentences\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "fit_text = list(train_X)\n",
    "fit_text.extend(list(pretrain_X))\n",
    "tokenizer.fit_on_texts(fit_text)\n",
    "\n",
    "pretrain_X = tokenizer.texts_to_sequences(pretrain_X)\n",
    "train_X = tokenizer.texts_to_sequences(train_X)\n",
    "val_X = tokenizer.texts_to_sequences(val_X)\n",
    "\n",
    "\n",
    "## Pad the sentences \n",
    "pretrain_X = pad_sequences(pretrain_X, maxlen=maxlen)\n",
    "train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "val_X = pad_sequences(val_X, maxlen=maxlen)\n",
    "\n",
    "\n",
    "import keras\n",
    "\n",
    "## Get the target values\n",
    "# train_y = train_df['stars'].apply(int)-1\n",
    "# val_y = val_df['stars'].apply(int)-1\n",
    "#test_y = test_df['stars'].apply(int)-1\n",
    "\n",
    "train_y = train_df[target].values\n",
    "pretrain_y = pretrain_df[\"label\"].values\n",
    "    \n",
    "val_y = val_df[target].values\n",
    "\n",
    "# train_y = keras.utils.to_categorical(train_y, num_classes=3)\n",
    "# val_y = keras.utils.to_categorical(val_y, num_classes=3)\n",
    "#test_y = keras.utils.to_categorical(test_y, num_classes=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2793\n",
      "311\n",
      "2793\n",
      "311\n"
     ]
    }
   ],
   "source": [
    "print(len(train_X))\n",
    "print(len(val_X))\n",
    "print(len(train_y))\n",
    "print(len(val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "e4c8211ccd5672607b26ec7d6470f7ab635e0334"
   },
   "outputs": [],
   "source": [
    "#shuffling the data\n",
    "np.random.seed(2019)\n",
    "trn_idx = np.random.permutation(len(train_X))\n",
    "val_idx = np.random.permutation(len(val_X))\n",
    "\n",
    "train_X = train_X[trn_idx]\n",
    "val_X = val_X[val_idx]\n",
    "train_y = train_y[trn_idx]\n",
    "val_y = val_y[val_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "a7c5ad7277ad7f33df9fbdd3243d68b9d7260e94"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xzhao\\Anaconda3\\envs\\Keras\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_FILE = 'glove.840B.300d.txt'\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding='utf-8'))\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= (max_features): continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "a588d1ab2113f12f2ef1937f0a6f6ad9ca3b802d"
   },
   "outputs": [],
   "source": [
    "import random, os, sys\n",
    "import numpy as np\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "from keras.initializers import *\n",
    "import tensorflow as tf\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "try:\n",
    "    from dataloader import TokenList, pad_to_longest\n",
    "    # for transformer\n",
    "except: pass\n",
    "\n",
    "class LayerNormalization(Layer):\n",
    "    def __init__(self, eps=1e-6, **kwargs):\n",
    "        self.eps = eps\n",
    "        super(LayerNormalization, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],\n",
    "                                     initializer=Ones(), trainable=True)\n",
    "        self.beta = self.add_weight(name='beta', shape=input_shape[-1:],\n",
    "                                    initializer=Zeros(), trainable=True)\n",
    "        super(LayerNormalization, self).build(input_shape)\n",
    "    def call(self, x):\n",
    "        mean = K.mean(x, axis=-1, keepdims=True)\n",
    "        std = K.std(x, axis=-1, keepdims=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "class ScaledDotProductAttention():\n",
    "    def __init__(self, d_model, attn_dropout=0.1):\n",
    "        self.temper = np.sqrt(d_model)\n",
    "        self.dropout = Dropout(attn_dropout)\n",
    "    def __call__(self, q, k, v, mask):\n",
    "        attn = Lambda(lambda x:K.batch_dot(x[0],x[1],axes=[2,2])/self.temper)([q, k])\n",
    "        if mask is not None:\n",
    "            mmask = Lambda(lambda x:(-1e+10)*(1-x))(mask)\n",
    "            attn = Add()([attn, mmask])\n",
    "        attn = Activation('softmax')(attn)\n",
    "        attn = self.dropout(attn)\n",
    "        output = Lambda(lambda x:K.batch_dot(x[0], x[1]))([attn, v])\n",
    "        return output, attn\n",
    "\n",
    "class MultiHeadAttention():\n",
    "    # mode 0 - big martixes, faster; mode 1 - more clear implementation\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout, mode=0, use_norm=True):\n",
    "        self.mode = mode\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.dropout = dropout\n",
    "        if mode == 0:\n",
    "            self.qs_layer = Dense(n_head*d_k, use_bias=False)\n",
    "            self.ks_layer = Dense(n_head*d_k, use_bias=False)\n",
    "            self.vs_layer = Dense(n_head*d_v, use_bias=False)\n",
    "        elif mode == 1:\n",
    "            self.qs_layers = []\n",
    "            self.ks_layers = []\n",
    "            self.vs_layers = []\n",
    "            for _ in range(n_head):\n",
    "                self.qs_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))\n",
    "                self.ks_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))\n",
    "                self.vs_layers.append(TimeDistributed(Dense(d_v, use_bias=False)))\n",
    "        self.attention = ScaledDotProductAttention(d_model)\n",
    "        self.layer_norm = LayerNormalization() if use_norm else None\n",
    "        self.w_o = TimeDistributed(Dense(d_model))\n",
    "\n",
    "    def __call__(self, q, k, v, mask=None):\n",
    "        d_k, d_v = self.d_k, self.d_v\n",
    "        n_head = self.n_head\n",
    "\n",
    "        if self.mode == 0:\n",
    "            qs = self.qs_layer(q)  # [batch_size, len_q, n_head*d_k]\n",
    "            ks = self.ks_layer(k)\n",
    "            vs = self.vs_layer(v)\n",
    "\n",
    "            def reshape1(x):\n",
    "                s = tf.shape(x)   # [batch_size, len_q, n_head * d_k]\n",
    "                x = tf.reshape(x, [s[0], s[1], n_head, d_k])\n",
    "                x = tf.transpose(x, [2, 0, 1, 3])  \n",
    "                x = tf.reshape(x, [-1, s[1], d_k])  # [n_head * batch_size, len_q, d_k]\n",
    "                return x\n",
    "            qs = Lambda(reshape1)(qs)\n",
    "            ks = Lambda(reshape1)(ks)\n",
    "            vs = Lambda(reshape1)(vs)\n",
    "\n",
    "            if mask is not None:\n",
    "                mask = Lambda(lambda x:K.repeat_elements(x, n_head, 0))(mask)\n",
    "            head, attn = self.attention(qs, ks, vs, mask=mask)  \n",
    "                \n",
    "            def reshape2(x):\n",
    "                s = tf.shape(x)   # [n_head * batch_size, len_v, d_v]\n",
    "                x = tf.reshape(x, [n_head, -1, s[1], s[2]]) \n",
    "                x = tf.transpose(x, [1, 2, 0, 3])\n",
    "                x = tf.reshape(x, [-1, s[1], n_head*d_v])  # [batch_size, len_v, n_head * d_v]\n",
    "                return x\n",
    "            head = Lambda(reshape2)(head)\n",
    "        elif self.mode == 1:\n",
    "            heads = []; attns = []\n",
    "            for i in range(n_head):\n",
    "                qs = self.qs_layers[i](q)   \n",
    "                ks = self.ks_layers[i](k) \n",
    "                vs = self.vs_layers[i](v) \n",
    "                head, attn = self.attention(qs, ks, vs, mask)\n",
    "                heads.append(head); attns.append(attn)\n",
    "            head = Concatenate()(heads) if n_head > 1 else heads[0]\n",
    "            attn = Concatenate()(attns) if n_head > 1 else attns[0]\n",
    "\n",
    "        outputs = self.w_o(head)\n",
    "        outputs = Dropout(self.dropout)(outputs)\n",
    "        if not self.layer_norm: return outputs, attn\n",
    "        # outputs = Add()([outputs, q]) # sl: fix\n",
    "        return self.layer_norm(outputs), attn\n",
    "\n",
    "class PositionwiseFeedForward():\n",
    "    def __init__(self, d_hid, d_inner_hid, dropout=0.1):\n",
    "        self.w_1 = Conv1D(d_inner_hid, 1, activation='relu')\n",
    "        self.w_2 = Conv1D(d_hid, 1)\n",
    "        self.layer_norm = LayerNormalization()\n",
    "        self.dropout = Dropout(dropout)\n",
    "    def __call__(self, x):\n",
    "        output = self.w_1(x) \n",
    "        output = self.w_2(output)\n",
    "        output = self.dropout(output)\n",
    "        output = Add()([output, x])\n",
    "        return self.layer_norm(output)\n",
    "\n",
    "class EncoderLayer():\n",
    "#class Encoder():\n",
    "    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=0.1):\n",
    "        self.self_att_layer = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.pos_ffn_layer  = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)\n",
    "    def __call__(self, enc_input, mask=None):\n",
    "        output, slf_attn = self.self_att_layer(enc_input, enc_input, enc_input, mask=mask)\n",
    "        output = self.pos_ffn_layer(output)\n",
    "        return output, slf_attn\n",
    "\n",
    "class Encoder():\n",
    "    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, \\\n",
    "                layers=6, dropout=0.1, word_emb=None, pos_emb=None):\n",
    "        self.emb_layer = word_emb\n",
    "        self.pos_layer = pos_emb\n",
    "        self.emb_dropout = Dropout(dropout)\n",
    "        self.layers = [EncoderLayer(d_model, d_inner_hid, n_head, d_k, d_v, dropout) for _ in range(layers)]\n",
    "    def __call__(self, src_seq, src_pos, return_att=False, active_layers=999):\n",
    "        x = self.emb_layer(src_seq)\n",
    "        if src_pos is not None:\n",
    "            pos = self.pos_layer(src_pos)\n",
    "            x = Add()([x, pos])\n",
    "        x = self.emb_dropout(x)\n",
    "        if return_att: atts = []\n",
    "        mask = Lambda(lambda x:GetPadMask(x, x))(src_seq)\n",
    "        for enc_layer in self.layers[:active_layers]:\n",
    "            x, att = enc_layer(x, mask)\n",
    "            if return_att: atts.append(att)\n",
    "        return (x, atts) if return_att else x\n",
    "    \n",
    "    \n",
    "\n",
    "def GetPosEncodingMatrix(max_len, d_emb):\n",
    "    pos_enc = np.array([\n",
    "        [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)] \n",
    "        if pos != 0 else np.zeros(d_emb) \n",
    "            for pos in range(max_len)\n",
    "            ])\n",
    "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2]) # dim 2i\n",
    "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2]) # dim 2i+1\n",
    "    return pos_enc\n",
    "\n",
    "def GetPadMask(q, k):\n",
    "    ones = K.expand_dims(K.ones_like(q, 'float32'), -1)\n",
    "    mask = K.cast(K.expand_dims(K.not_equal(k, 0), 1), 'float32')\n",
    "    mask = K.batch_dot(ones, mask, axes=[2,1])\n",
    "    return mask\n",
    "\n",
    "def GetSubMask(s):\n",
    "    len_s = tf.shape(s)[1]\n",
    "    bs = tf.shape(s)[:1]\n",
    "    mask = K.cumsum(tf.eye(len_s, batch_shape=bs), 1)\n",
    "    return mask\n",
    "\n",
    "class Transformer():\n",
    "    def __init__(self, len_limit, embedding_matrix, d_model=embed_size, \\\n",
    "              d_inner_hid=512, n_head=10, d_k=64, d_v=64, layers=2, dropout=0.1, \\\n",
    "              share_word_emb=False, **kwargs):\n",
    "        self.name = 'Transformer'\n",
    "        self.len_limit = len_limit\n",
    "        self.src_loc_info = False # True # sl: fix later\n",
    "        self.d_model = d_model\n",
    "        self.decode_model = None\n",
    "        d_emb = d_model\n",
    "\n",
    "        pos_emb = Embedding(len_limit, d_emb, trainable=False, \\\n",
    "                            weights=[GetPosEncodingMatrix(len_limit, d_emb)])\n",
    "\n",
    "        i_word_emb = Embedding(max_features, d_emb, weights=[embedding_matrix]) # Add Kaggle provided embedding here\n",
    "        # i_word_emb = Embedding(68976, d_emb, weights=[embedding_matrix])\n",
    "        \n",
    "        self.encoder = Encoder(d_model, d_inner_hid, n_head, d_k, d_v, layers, dropout, \\\n",
    "                               word_emb=i_word_emb, pos_emb=pos_emb)\n",
    "\n",
    "        \n",
    "    def get_pos_seq(self, x):\n",
    "        mask = K.cast(K.not_equal(x, 0), 'int32')\n",
    "        pos = K.cumsum(K.ones_like(x, 'int32'), 1)\n",
    "        return pos * mask\n",
    "\n",
    "    def compile(self, active_layers=999):\n",
    "        src_seq_input = Input(shape=(None, ))\n",
    "        x = Embedding(max_features, embed_size, weights=[embedding_matrix])(src_seq_input)\n",
    "        # x = Embedding(65359, embed_size, weights=[embedding_matrix])(src_seq_input)\n",
    "        \n",
    "        # LSTM before attention layers\n",
    "        # x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\n",
    "        x = Bidirectional(CuDNNLSTM(300, return_sequences=True))(x)\n",
    "        x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x) \n",
    "        \n",
    "        x, slf_attn = MultiHeadAttention(n_head=3, d_model=300, d_k=64, d_v=64, dropout=0.1)(x, x, x)\n",
    "        \n",
    "        avg_pool = GlobalAveragePooling1D()(x)\n",
    "        max_pool = GlobalMaxPooling1D()(x)\n",
    "        conc = concatenate([avg_pool, max_pool])\n",
    "        conc = Dense(64, activation=\"relu\")(conc)\n",
    "        x = Dense(1, activation=\"sigmoid\")(conc)   \n",
    "        \n",
    "        self.model = Model(inputs=src_seq_input, outputs=x)\n",
    "        # self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "370eb8437768b36204404e712692e48746b76476",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\xzhao\\Anaconda3\\envs\\Keras\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\xzhao\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, None, 300)    20400000    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, None, 600)    1444800     embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, None, 128)    340992      bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, None, 192)    24576       bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, None, 192)    24576       bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None, 64)     0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, None, 64)     0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, None, None)   0           lambda_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, None)   0           lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, None, 192)    24576       bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, None, None)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, None, 64)     0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, None, 64)     0           dropout_4[0][0]                  \n",
      "                                                                 lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, None, 192)    0           lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, None, 300)    57900       lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, None, 300)    0           time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, None, 300)    600         dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 300)          0           layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 300)          0           layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 600)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 64)           38464       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 1)            65          dense_9[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 22,356,549\n",
      "Trainable params: 22,356,549\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "s2s = Transformer(64, embedding_matrix, layers=1)\n",
    "s2s.compile()\n",
    "model = s2s.model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "dd4ba6a5810c9c4220c491d84b91557d32a649ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\xzhao\\Anaconda3\\envs\\Keras\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\xzhao\\Anaconda3\\envs\\Keras\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 4879 samples, validate on 311 samples\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4140/4879 [========================>.....] - ETA: 20:39 - loss: 0.8047 - acc: 0.50 - ETA: 11:27 - loss: 0.8446 - acc: 0.55 - ETA: 8:23 - loss: 0.8642 - acc: 0.5167 - ETA: 6:51 - loss: 0.9041 - acc: 0.512 - ETA: 5:55 - loss: 0.8801 - acc: 0.500 - ETA: 5:17 - loss: 0.8341 - acc: 0.533 - ETA: 4:50 - loss: 0.8662 - acc: 0.521 - ETA: 4:30 - loss: 0.8467 - acc: 0.537 - ETA: 4:14 - loss: 0.8230 - acc: 0.550 - ETA: 4:01 - loss: 0.8240 - acc: 0.555 - ETA: 3:51 - loss: 0.8615 - acc: 0.536 - ETA: 3:42 - loss: 0.8651 - acc: 0.525 - ETA: 3:34 - loss: 0.8471 - acc: 0.530 - ETA: 3:28 - loss: 0.8367 - acc: 0.532 - ETA: 3:22 - loss: 0.8289 - acc: 0.530 - ETA: 3:17 - loss: 0.8163 - acc: 0.540 - ETA: 3:12 - loss: 0.8118 - acc: 0.526 - ETA: 3:08 - loss: 0.8067 - acc: 0.519 - ETA: 3:04 - loss: 0.7987 - acc: 0.526 - ETA: 3:01 - loss: 0.7974 - acc: 0.520 - ETA: 2:58 - loss: 0.7924 - acc: 0.521 - ETA: 2:55 - loss: 0.7873 - acc: 0.520 - ETA: 2:52 - loss: 0.7808 - acc: 0.523 - ETA: 2:49 - loss: 0.7767 - acc: 0.531 - ETA: 2:47 - loss: 0.7775 - acc: 0.528 - ETA: 2:44 - loss: 0.7753 - acc: 0.525 - ETA: 2:42 - loss: 0.7704 - acc: 0.527 - ETA: 2:40 - loss: 0.7681 - acc: 0.526 - ETA: 2:38 - loss: 0.7714 - acc: 0.524 - ETA: 2:36 - loss: 0.7695 - acc: 0.526 - ETA: 2:35 - loss: 0.7686 - acc: 0.524 - ETA: 2:33 - loss: 0.7622 - acc: 0.531 - ETA: 2:31 - loss: 0.7589 - acc: 0.536 - ETA: 2:30 - loss: 0.7561 - acc: 0.539 - ETA: 2:28 - loss: 0.7538 - acc: 0.542 - ETA: 2:27 - loss: 0.7531 - acc: 0.543 - ETA: 2:26 - loss: 0.7507 - acc: 0.545 - ETA: 2:24 - loss: 0.7485 - acc: 0.547 - ETA: 2:23 - loss: 0.7488 - acc: 0.544 - ETA: 2:22 - loss: 0.7464 - acc: 0.548 - ETA: 2:20 - loss: 0.7473 - acc: 0.545 - ETA: 2:19 - loss: 0.7446 - acc: 0.548 - ETA: 2:18 - loss: 0.7434 - acc: 0.547 - ETA: 2:17 - loss: 0.7428 - acc: 0.546 - ETA: 2:16 - loss: 0.7414 - acc: 0.547 - ETA: 2:15 - loss: 0.7404 - acc: 0.544 - ETA: 2:13 - loss: 0.7391 - acc: 0.544 - ETA: 2:13 - loss: 0.7374 - acc: 0.549 - ETA: 2:11 - loss: 0.7345 - acc: 0.553 - ETA: 2:10 - loss: 0.7331 - acc: 0.552 - ETA: 2:09 - loss: 0.7324 - acc: 0.552 - ETA: 2:08 - loss: 0.7296 - acc: 0.554 - ETA: 2:07 - loss: 0.7289 - acc: 0.557 - ETA: 2:06 - loss: 0.7264 - acc: 0.560 - ETA: 2:06 - loss: 0.7233 - acc: 0.564 - ETA: 2:05 - loss: 0.7250 - acc: 0.563 - ETA: 2:04 - loss: 0.7246 - acc: 0.564 - ETA: 2:03 - loss: 0.7240 - acc: 0.564 - ETA: 2:02 - loss: 0.7231 - acc: 0.565 - ETA: 2:01 - loss: 0.7229 - acc: 0.566 - ETA: 2:00 - loss: 0.7216 - acc: 0.568 - ETA: 1:59 - loss: 0.7194 - acc: 0.570 - ETA: 1:58 - loss: 0.7190 - acc: 0.570 - ETA: 1:58 - loss: 0.7191 - acc: 0.570 - ETA: 1:57 - loss: 0.7185 - acc: 0.571 - ETA: 1:56 - loss: 0.7166 - acc: 0.573 - ETA: 1:55 - loss: 0.7170 - acc: 0.572 - ETA: 1:54 - loss: 0.7171 - acc: 0.571 - ETA: 1:53 - loss: 0.7177 - acc: 0.568 - ETA: 1:53 - loss: 0.7165 - acc: 0.570 - ETA: 1:52 - loss: 0.7166 - acc: 0.568 - ETA: 1:51 - loss: 0.7163 - acc: 0.567 - ETA: 1:50 - loss: 0.7160 - acc: 0.566 - ETA: 1:49 - loss: 0.7153 - acc: 0.566 - ETA: 1:49 - loss: 0.7145 - acc: 0.568 - ETA: 1:48 - loss: 0.7148 - acc: 0.565 - ETA: 1:47 - loss: 0.7145 - acc: 0.566 - ETA: 1:46 - loss: 0.7143 - acc: 0.563 - ETA: 1:46 - loss: 0.7134 - acc: 0.564 - ETA: 1:45 - loss: 0.7135 - acc: 0.563 - ETA: 1:44 - loss: 0.7130 - acc: 0.563 - ETA: 1:43 - loss: 0.7122 - acc: 0.564 - ETA: 1:43 - loss: 0.7120 - acc: 0.565 - ETA: 1:42 - loss: 0.7113 - acc: 0.566 - ETA: 1:41 - loss: 0.7105 - acc: 0.568 - ETA: 1:41 - loss: 0.7103 - acc: 0.569 - ETA: 1:40 - loss: 0.7090 - acc: 0.569 - ETA: 1:39 - loss: 0.7093 - acc: 0.569 - ETA: 1:38 - loss: 0.7093 - acc: 0.569 - ETA: 1:38 - loss: 0.7090 - acc: 0.569 - ETA: 1:37 - loss: 0.7101 - acc: 0.567 - ETA: 1:36 - loss: 0.7100 - acc: 0.566 - ETA: 1:35 - loss: 0.7084 - acc: 0.569 - ETA: 1:35 - loss: 0.7082 - acc: 0.570 - ETA: 1:34 - loss: 0.7058 - acc: 0.572 - ETA: 1:33 - loss: 0.7064 - acc: 0.570 - ETA: 1:33 - loss: 0.7094 - acc: 0.570 - ETA: 1:32 - loss: 0.7148 - acc: 0.568 - ETA: 1:31 - loss: 0.7162 - acc: 0.567 - ETA: 1:31 - loss: 0.7154 - acc: 0.568 - ETA: 1:30 - loss: 0.7143 - acc: 0.569 - ETA: 1:29 - loss: 0.7144 - acc: 0.569 - ETA: 1:28 - loss: 0.7140 - acc: 0.569 - ETA: 1:28 - loss: 0.7136 - acc: 0.570 - ETA: 1:27 - loss: 0.7158 - acc: 0.569 - ETA: 1:26 - loss: 0.7153 - acc: 0.570 - ETA: 1:26 - loss: 0.7180 - acc: 0.567 - ETA: 1:25 - loss: 0.7185 - acc: 0.565 - ETA: 1:24 - loss: 0.7184 - acc: 0.564 - ETA: 1:24 - loss: 0.7189 - acc: 0.563 - ETA: 1:23 - loss: 0.7192 - acc: 0.563 - ETA: 1:22 - loss: 0.7195 - acc: 0.563 - ETA: 1:22 - loss: 0.7191 - acc: 0.564 - ETA: 1:21 - loss: 0.7190 - acc: 0.564 - ETA: 1:20 - loss: 0.7203 - acc: 0.563 - ETA: 1:20 - loss: 0.7207 - acc: 0.562 - ETA: 1:19 - loss: 0.7199 - acc: 0.563 - ETA: 1:18 - loss: 0.7190 - acc: 0.564 - ETA: 1:18 - loss: 0.7188 - acc: 0.563 - ETA: 1:17 - loss: 0.7188 - acc: 0.562 - ETA: 1:16 - loss: 0.7190 - acc: 0.561 - ETA: 1:16 - loss: 0.7187 - acc: 0.560 - ETA: 1:15 - loss: 0.7190 - acc: 0.559 - ETA: 1:14 - loss: 0.7188 - acc: 0.558 - ETA: 1:14 - loss: 0.7184 - acc: 0.559 - ETA: 1:13 - loss: 0.7178 - acc: 0.559 - ETA: 1:12 - loss: 0.7178 - acc: 0.557 - ETA: 1:12 - loss: 0.7175 - acc: 0.557 - ETA: 1:11 - loss: 0.7168 - acc: 0.558 - ETA: 1:10 - loss: 0.7163 - acc: 0.558 - ETA: 1:10 - loss: 0.7157 - acc: 0.559 - ETA: 1:09 - loss: 0.7155 - acc: 0.558 - ETA: 1:09 - loss: 0.7148 - acc: 0.559 - ETA: 1:08 - loss: 0.7144 - acc: 0.559 - ETA: 1:07 - loss: 0.7137 - acc: 0.560 - ETA: 1:07 - loss: 0.7142 - acc: 0.559 - ETA: 1:06 - loss: 0.7137 - acc: 0.559 - ETA: 1:05 - loss: 0.7133 - acc: 0.559 - ETA: 1:05 - loss: 0.7137 - acc: 0.559 - ETA: 1:04 - loss: 0.7131 - acc: 0.560 - ETA: 1:03 - loss: 0.7116 - acc: 0.562 - ETA: 1:03 - loss: 0.7115 - acc: 0.562 - ETA: 1:02 - loss: 0.7119 - acc: 0.561 - ETA: 1:01 - loss: 0.7114 - acc: 0.561 - ETA: 1:01 - loss: 0.7113 - acc: 0.561 - ETA: 1:00 - loss: 0.7113 - acc: 0.560 - ETA: 1:00 - loss: 0.7107 - acc: 0.560 - ETA: 59s - loss: 0.7100 - acc: 0.561 - ETA: 58s - loss: 0.7092 - acc: 0.56 - ETA: 58s - loss: 0.7093 - acc: 0.56 - ETA: 57s - loss: 0.7090 - acc: 0.56 - ETA: 56s - loss: 0.7080 - acc: 0.56 - ETA: 56s - loss: 0.7070 - acc: 0.56 - ETA: 55s - loss: 0.7064 - acc: 0.56 - ETA: 54s - loss: 0.7058 - acc: 0.56 - ETA: 54s - loss: 0.7054 - acc: 0.56 - ETA: 53s - loss: 0.7055 - acc: 0.56 - ETA: 53s - loss: 0.7045 - acc: 0.56 - ETA: 52s - loss: 0.7051 - acc: 0.56 - ETA: 51s - loss: 0.7050 - acc: 0.56 - ETA: 51s - loss: 0.7057 - acc: 0.56 - ETA: 50s - loss: 0.7050 - acc: 0.56 - ETA: 49s - loss: 0.7048 - acc: 0.56 - ETA: 49s - loss: 0.7043 - acc: 0.56 - ETA: 48s - loss: 0.7037 - acc: 0.56 - ETA: 47s - loss: 0.7032 - acc: 0.56 - ETA: 47s - loss: 0.7036 - acc: 0.56 - ETA: 46s - loss: 0.7025 - acc: 0.56 - ETA: 46s - loss: 0.7021 - acc: 0.56 - ETA: 45s - loss: 0.7011 - acc: 0.57 - ETA: 44s - loss: 0.7014 - acc: 0.57 - ETA: 44s - loss: 0.7013 - acc: 0.57 - ETA: 43s - loss: 0.7014 - acc: 0.57 - ETA: 42s - loss: 0.7007 - acc: 0.57 - ETA: 42s - loss: 0.7005 - acc: 0.57 - ETA: 41s - loss: 0.7002 - acc: 0.57 - ETA: 41s - loss: 0.6999 - acc: 0.57 - ETA: 40s - loss: 0.7004 - acc: 0.57 - ETA: 39s - loss: 0.7004 - acc: 0.57 - ETA: 39s - loss: 0.7000 - acc: 0.57 - ETA: 38s - loss: 0.7002 - acc: 0.57 - ETA: 38s - loss: 0.7002 - acc: 0.57 - ETA: 37s - loss: 0.7000 - acc: 0.57 - ETA: 36s - loss: 0.6996 - acc: 0.57 - ETA: 36s - loss: 0.6993 - acc: 0.57 - ETA: 35s - loss: 0.6993 - acc: 0.57 - ETA: 34s - loss: 0.6992 - acc: 0.57 - ETA: 34s - loss: 0.6987 - acc: 0.57 - ETA: 33s - loss: 0.6979 - acc: 0.57 - ETA: 33s - loss: 0.6979 - acc: 0.57 - ETA: 32s - loss: 0.6972 - acc: 0.57 - ETA: 31s - loss: 0.6970 - acc: 0.57 - ETA: 31s - loss: 0.6966 - acc: 0.57 - ETA: 30s - loss: 0.6963 - acc: 0.57 - ETA: 29s - loss: 0.6954 - acc: 0.57 - ETA: 29s - loss: 0.6951 - acc: 0.57 - ETA: 28s - loss: 0.6950 - acc: 0.57 - ETA: 28s - loss: 0.6948 - acc: 0.57 - ETA: 27s - loss: 0.6942 - acc: 0.57 - ETA: 26s - loss: 0.6940 - acc: 0.57 - ETA: 26s - loss: 0.6935 - acc: 0.57 - ETA: 25s - loss: 0.6934 - acc: 0.57 - ETA: 25s - loss: 0.6936 - acc: 0.57 - ETA: 24s - loss: 0.6936 - acc: 0.57 - ETA: 23s - loss: 0.6936 - acc: 0.57 - ETA: 23s - loss: 0.6932 - acc: 0.57 - ETA: 22s - loss: 0.6941 - acc: 0.5785"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4879/4879 [==============================] - ETA: 21s - loss: 0.6938 - acc: 0.57 - ETA: 21s - loss: 0.6935 - acc: 0.57 - ETA: 20s - loss: 0.6934 - acc: 0.57 - ETA: 20s - loss: 0.6931 - acc: 0.57 - ETA: 19s - loss: 0.6934 - acc: 0.57 - ETA: 18s - loss: 0.6935 - acc: 0.57 - ETA: 18s - loss: 0.6937 - acc: 0.57 - ETA: 17s - loss: 0.6932 - acc: 0.57 - ETA: 17s - loss: 0.6928 - acc: 0.57 - ETA: 16s - loss: 0.6926 - acc: 0.57 - ETA: 15s - loss: 0.6922 - acc: 0.57 - ETA: 15s - loss: 0.6924 - acc: 0.57 - ETA: 14s - loss: 0.6930 - acc: 0.57 - ETA: 14s - loss: 0.6930 - acc: 0.57 - ETA: 13s - loss: 0.6930 - acc: 0.57 - ETA: 12s - loss: 0.6928 - acc: 0.57 - ETA: 12s - loss: 0.6927 - acc: 0.57 - ETA: 11s - loss: 0.6922 - acc: 0.57 - ETA: 10s - loss: 0.6919 - acc: 0.57 - ETA: 10s - loss: 0.6917 - acc: 0.57 - ETA: 9s - loss: 0.6917 - acc: 0.5774 - ETA: 9s - loss: 0.6917 - acc: 0.577 - ETA: 8s - loss: 0.6916 - acc: 0.576 - ETA: 7s - loss: 0.6918 - acc: 0.575 - ETA: 7s - loss: 0.6918 - acc: 0.576 - ETA: 6s - loss: 0.6912 - acc: 0.577 - ETA: 6s - loss: 0.6913 - acc: 0.577 - ETA: 5s - loss: 0.6909 - acc: 0.578 - ETA: 4s - loss: 0.6910 - acc: 0.578 - ETA: 4s - loss: 0.6911 - acc: 0.577 - ETA: 3s - loss: 0.6906 - acc: 0.578 - ETA: 3s - loss: 0.6909 - acc: 0.578 - ETA: 2s - loss: 0.6906 - acc: 0.578 - ETA: 1s - loss: 0.6908 - acc: 0.578 - ETA: 1s - loss: 0.6906 - acc: 0.578 - ETA: 0s - loss: 0.6906 - acc: 0.579 - 152s 31ms/step - loss: 0.6908 - acc: 0.5786 - val_loss: 0.7857 - val_acc: 0.5145\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4140/4879 [========================>.....] - ETA: 2:23 - loss: 0.6610 - acc: 0.650 - ETA: 2:22 - loss: 0.6470 - acc: 0.675 - ETA: 2:22 - loss: 0.6721 - acc: 0.633 - ETA: 2:21 - loss: 0.6583 - acc: 0.650 - ETA: 2:21 - loss: 0.6419 - acc: 0.670 - ETA: 2:20 - loss: 0.6556 - acc: 0.641 - ETA: 2:20 - loss: 0.6533 - acc: 0.642 - ETA: 2:20 - loss: 0.6552 - acc: 0.637 - ETA: 2:19 - loss: 0.6551 - acc: 0.633 - ETA: 2:19 - loss: 0.6605 - acc: 0.625 - ETA: 2:18 - loss: 0.6611 - acc: 0.622 - ETA: 2:18 - loss: 0.6598 - acc: 0.625 - ETA: 2:18 - loss: 0.6597 - acc: 0.623 - ETA: 2:17 - loss: 0.6607 - acc: 0.621 - ETA: 2:16 - loss: 0.6626 - acc: 0.613 - ETA: 2:16 - loss: 0.6586 - acc: 0.621 - ETA: 2:15 - loss: 0.6614 - acc: 0.617 - ETA: 2:14 - loss: 0.6697 - acc: 0.600 - ETA: 2:14 - loss: 0.6702 - acc: 0.600 - ETA: 2:13 - loss: 0.6695 - acc: 0.597 - ETA: 2:12 - loss: 0.6708 - acc: 0.585 - ETA: 2:12 - loss: 0.6730 - acc: 0.579 - ETA: 2:11 - loss: 0.6718 - acc: 0.580 - ETA: 2:11 - loss: 0.6762 - acc: 0.572 - ETA: 2:10 - loss: 0.6778 - acc: 0.572 - ETA: 2:10 - loss: 0.6782 - acc: 0.569 - ETA: 2:09 - loss: 0.6794 - acc: 0.564 - ETA: 2:08 - loss: 0.6804 - acc: 0.560 - ETA: 2:08 - loss: 0.6803 - acc: 0.555 - ETA: 2:07 - loss: 0.6798 - acc: 0.555 - ETA: 2:07 - loss: 0.6780 - acc: 0.559 - ETA: 2:06 - loss: 0.6789 - acc: 0.559 - ETA: 2:05 - loss: 0.6760 - acc: 0.565 - ETA: 2:05 - loss: 0.6751 - acc: 0.567 - ETA: 2:04 - loss: 0.6743 - acc: 0.570 - ETA: 2:04 - loss: 0.6695 - acc: 0.576 - ETA: 2:03 - loss: 0.6748 - acc: 0.573 - ETA: 2:03 - loss: 0.6717 - acc: 0.577 - ETA: 2:02 - loss: 0.6747 - acc: 0.573 - ETA: 2:01 - loss: 0.6747 - acc: 0.575 - ETA: 2:01 - loss: 0.6727 - acc: 0.578 - ETA: 2:00 - loss: 0.6740 - acc: 0.576 - ETA: 2:00 - loss: 0.6729 - acc: 0.577 - ETA: 1:59 - loss: 0.6726 - acc: 0.579 - ETA: 1:58 - loss: 0.6755 - acc: 0.576 - ETA: 1:58 - loss: 0.6748 - acc: 0.578 - ETA: 1:57 - loss: 0.6736 - acc: 0.579 - ETA: 1:57 - loss: 0.6764 - acc: 0.576 - ETA: 1:56 - loss: 0.6768 - acc: 0.574 - ETA: 1:55 - loss: 0.6774 - acc: 0.570 - ETA: 1:55 - loss: 0.6747 - acc: 0.572 - ETA: 1:54 - loss: 0.6758 - acc: 0.569 - ETA: 1:54 - loss: 0.6788 - acc: 0.564 - ETA: 1:53 - loss: 0.6796 - acc: 0.563 - ETA: 1:52 - loss: 0.6792 - acc: 0.563 - ETA: 1:52 - loss: 0.6791 - acc: 0.562 - ETA: 1:51 - loss: 0.6790 - acc: 0.563 - ETA: 1:51 - loss: 0.6791 - acc: 0.562 - ETA: 1:50 - loss: 0.6779 - acc: 0.566 - ETA: 1:49 - loss: 0.6781 - acc: 0.565 - ETA: 1:49 - loss: 0.6793 - acc: 0.564 - ETA: 1:48 - loss: 0.6801 - acc: 0.563 - ETA: 1:48 - loss: 0.6808 - acc: 0.562 - ETA: 1:47 - loss: 0.6831 - acc: 0.558 - ETA: 1:46 - loss: 0.6835 - acc: 0.557 - ETA: 1:46 - loss: 0.6832 - acc: 0.559 - ETA: 1:45 - loss: 0.6832 - acc: 0.559 - ETA: 1:45 - loss: 0.6834 - acc: 0.561 - ETA: 1:44 - loss: 0.6827 - acc: 0.563 - ETA: 1:43 - loss: 0.6824 - acc: 0.563 - ETA: 1:43 - loss: 0.6822 - acc: 0.564 - ETA: 1:42 - loss: 0.6827 - acc: 0.563 - ETA: 1:42 - loss: 0.6838 - acc: 0.561 - ETA: 1:41 - loss: 0.6853 - acc: 0.559 - ETA: 1:40 - loss: 0.6859 - acc: 0.557 - ETA: 1:40 - loss: 0.6872 - acc: 0.554 - ETA: 1:39 - loss: 0.6872 - acc: 0.554 - ETA: 1:39 - loss: 0.6869 - acc: 0.555 - ETA: 1:38 - loss: 0.6867 - acc: 0.555 - ETA: 1:37 - loss: 0.6858 - acc: 0.556 - ETA: 1:37 - loss: 0.6859 - acc: 0.556 - ETA: 1:36 - loss: 0.6871 - acc: 0.555 - ETA: 1:36 - loss: 0.6861 - acc: 0.557 - ETA: 1:35 - loss: 0.6863 - acc: 0.558 - ETA: 1:35 - loss: 0.6888 - acc: 0.555 - ETA: 1:34 - loss: 0.6876 - acc: 0.557 - ETA: 1:33 - loss: 0.6888 - acc: 0.556 - ETA: 1:33 - loss: 0.6885 - acc: 0.556 - ETA: 1:32 - loss: 0.6885 - acc: 0.557 - ETA: 1:32 - loss: 0.6886 - acc: 0.557 - ETA: 1:31 - loss: 0.6882 - acc: 0.556 - ETA: 1:30 - loss: 0.6889 - acc: 0.556 - ETA: 1:30 - loss: 0.6894 - acc: 0.555 - ETA: 1:29 - loss: 0.6893 - acc: 0.555 - ETA: 1:29 - loss: 0.6888 - acc: 0.555 - ETA: 1:28 - loss: 0.6879 - acc: 0.555 - ETA: 1:27 - loss: 0.6892 - acc: 0.553 - ETA: 1:27 - loss: 0.6888 - acc: 0.554 - ETA: 1:26 - loss: 0.6892 - acc: 0.554 - ETA: 1:26 - loss: 0.6887 - acc: 0.554 - ETA: 1:25 - loss: 0.6891 - acc: 0.553 - ETA: 1:24 - loss: 0.6888 - acc: 0.554 - ETA: 1:24 - loss: 0.6877 - acc: 0.557 - ETA: 1:23 - loss: 0.6873 - acc: 0.557 - ETA: 1:23 - loss: 0.6865 - acc: 0.558 - ETA: 1:22 - loss: 0.6861 - acc: 0.559 - ETA: 1:21 - loss: 0.6855 - acc: 0.560 - ETA: 1:21 - loss: 0.6850 - acc: 0.562 - ETA: 1:20 - loss: 0.6839 - acc: 0.564 - ETA: 1:20 - loss: 0.6844 - acc: 0.565 - ETA: 1:19 - loss: 0.6848 - acc: 0.565 - ETA: 1:18 - loss: 0.6853 - acc: 0.563 - ETA: 1:18 - loss: 0.6853 - acc: 0.563 - ETA: 1:17 - loss: 0.6845 - acc: 0.563 - ETA: 1:17 - loss: 0.6839 - acc: 0.564 - ETA: 1:16 - loss: 0.6837 - acc: 0.564 - ETA: 1:15 - loss: 0.6846 - acc: 0.563 - ETA: 1:15 - loss: 0.6841 - acc: 0.564 - ETA: 1:14 - loss: 0.6826 - acc: 0.567 - ETA: 1:14 - loss: 0.6820 - acc: 0.568 - ETA: 1:13 - loss: 0.6821 - acc: 0.568 - ETA: 1:12 - loss: 0.6819 - acc: 0.566 - ETA: 1:12 - loss: 0.6818 - acc: 0.567 - ETA: 1:11 - loss: 0.6819 - acc: 0.566 - ETA: 1:11 - loss: 0.6818 - acc: 0.567 - ETA: 1:10 - loss: 0.6818 - acc: 0.567 - ETA: 1:09 - loss: 0.6829 - acc: 0.566 - ETA: 1:09 - loss: 0.6815 - acc: 0.568 - ETA: 1:08 - loss: 0.6808 - acc: 0.570 - ETA: 1:08 - loss: 0.6810 - acc: 0.569 - ETA: 1:07 - loss: 0.6806 - acc: 0.570 - ETA: 1:06 - loss: 0.6806 - acc: 0.570 - ETA: 1:06 - loss: 0.6802 - acc: 0.571 - ETA: 1:05 - loss: 0.6806 - acc: 0.571 - ETA: 1:05 - loss: 0.6806 - acc: 0.571 - ETA: 1:04 - loss: 0.6803 - acc: 0.572 - ETA: 1:03 - loss: 0.6805 - acc: 0.571 - ETA: 1:03 - loss: 0.6798 - acc: 0.573 - ETA: 1:02 - loss: 0.6796 - acc: 0.573 - ETA: 1:02 - loss: 0.6788 - acc: 0.574 - ETA: 1:01 - loss: 0.6793 - acc: 0.573 - ETA: 1:00 - loss: 0.6793 - acc: 0.573 - ETA: 1:00 - loss: 0.6792 - acc: 0.573 - ETA: 59s - loss: 0.6795 - acc: 0.572 - ETA: 59s - loss: 0.6799 - acc: 0.57 - ETA: 58s - loss: 0.6794 - acc: 0.57 - ETA: 57s - loss: 0.6792 - acc: 0.57 - ETA: 57s - loss: 0.6787 - acc: 0.57 - ETA: 56s - loss: 0.6789 - acc: 0.57 - ETA: 56s - loss: 0.6786 - acc: 0.57 - ETA: 55s - loss: 0.6785 - acc: 0.57 - ETA: 54s - loss: 0.6787 - acc: 0.57 - ETA: 54s - loss: 0.6783 - acc: 0.57 - ETA: 53s - loss: 0.6777 - acc: 0.57 - ETA: 53s - loss: 0.6771 - acc: 0.57 - ETA: 52s - loss: 0.6772 - acc: 0.57 - ETA: 51s - loss: 0.6767 - acc: 0.57 - ETA: 51s - loss: 0.6764 - acc: 0.57 - ETA: 50s - loss: 0.6759 - acc: 0.57 - ETA: 50s - loss: 0.6760 - acc: 0.57 - ETA: 49s - loss: 0.6758 - acc: 0.57 - ETA: 48s - loss: 0.6753 - acc: 0.57 - ETA: 48s - loss: 0.6743 - acc: 0.57 - ETA: 47s - loss: 0.6731 - acc: 0.57 - ETA: 47s - loss: 0.6731 - acc: 0.57 - ETA: 46s - loss: 0.6725 - acc: 0.58 - ETA: 45s - loss: 0.6734 - acc: 0.58 - ETA: 45s - loss: 0.6732 - acc: 0.58 - ETA: 44s - loss: 0.6735 - acc: 0.57 - ETA: 44s - loss: 0.6728 - acc: 0.58 - ETA: 43s - loss: 0.6718 - acc: 0.58 - ETA: 43s - loss: 0.6721 - acc: 0.58 - ETA: 42s - loss: 0.6724 - acc: 0.58 - ETA: 41s - loss: 0.6722 - acc: 0.58 - ETA: 41s - loss: 0.6719 - acc: 0.58 - ETA: 40s - loss: 0.6708 - acc: 0.58 - ETA: 40s - loss: 0.6699 - acc: 0.58 - ETA: 39s - loss: 0.6701 - acc: 0.58 - ETA: 38s - loss: 0.6695 - acc: 0.58 - ETA: 38s - loss: 0.6695 - acc: 0.58 - ETA: 37s - loss: 0.6688 - acc: 0.58 - ETA: 37s - loss: 0.6693 - acc: 0.58 - ETA: 36s - loss: 0.6707 - acc: 0.58 - ETA: 35s - loss: 0.6712 - acc: 0.58 - ETA: 35s - loss: 0.6708 - acc: 0.58 - ETA: 34s - loss: 0.6716 - acc: 0.58 - ETA: 34s - loss: 0.6718 - acc: 0.58 - ETA: 33s - loss: 0.6716 - acc: 0.58 - ETA: 32s - loss: 0.6718 - acc: 0.58 - ETA: 32s - loss: 0.6719 - acc: 0.58 - ETA: 31s - loss: 0.6720 - acc: 0.58 - ETA: 31s - loss: 0.6721 - acc: 0.58 - ETA: 30s - loss: 0.6721 - acc: 0.58 - ETA: 29s - loss: 0.6721 - acc: 0.58 - ETA: 29s - loss: 0.6722 - acc: 0.58 - ETA: 28s - loss: 0.6724 - acc: 0.58 - ETA: 28s - loss: 0.6729 - acc: 0.58 - ETA: 27s - loss: 0.6736 - acc: 0.57 - ETA: 26s - loss: 0.6736 - acc: 0.57 - ETA: 26s - loss: 0.6736 - acc: 0.57 - ETA: 25s - loss: 0.6737 - acc: 0.57 - ETA: 25s - loss: 0.6737 - acc: 0.57 - ETA: 24s - loss: 0.6737 - acc: 0.57 - ETA: 23s - loss: 0.6739 - acc: 0.57 - ETA: 23s - loss: 0.6739 - acc: 0.57 - ETA: 22s - loss: 0.6739 - acc: 0.57 - ETA: 22s - loss: 0.6739 - acc: 0.5787"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4879/4879 [==============================] - ETA: 21s - loss: 0.6738 - acc: 0.57 - ETA: 20s - loss: 0.6737 - acc: 0.57 - ETA: 20s - loss: 0.6737 - acc: 0.57 - ETA: 19s - loss: 0.6735 - acc: 0.58 - ETA: 19s - loss: 0.6736 - acc: 0.58 - ETA: 18s - loss: 0.6734 - acc: 0.58 - ETA: 17s - loss: 0.6737 - acc: 0.57 - ETA: 17s - loss: 0.6734 - acc: 0.58 - ETA: 16s - loss: 0.6733 - acc: 0.58 - ETA: 16s - loss: 0.6736 - acc: 0.58 - ETA: 15s - loss: 0.6733 - acc: 0.58 - ETA: 14s - loss: 0.6737 - acc: 0.58 - ETA: 14s - loss: 0.6736 - acc: 0.58 - ETA: 13s - loss: 0.6734 - acc: 0.58 - ETA: 13s - loss: 0.6730 - acc: 0.58 - ETA: 12s - loss: 0.6729 - acc: 0.58 - ETA: 11s - loss: 0.6726 - acc: 0.58 - ETA: 11s - loss: 0.6726 - acc: 0.58 - ETA: 10s - loss: 0.6726 - acc: 0.58 - ETA: 10s - loss: 0.6728 - acc: 0.58 - ETA: 9s - loss: 0.6727 - acc: 0.5811 - ETA: 8s - loss: 0.6729 - acc: 0.581 - ETA: 8s - loss: 0.6732 - acc: 0.580 - ETA: 7s - loss: 0.6733 - acc: 0.580 - ETA: 7s - loss: 0.6738 - acc: 0.580 - ETA: 6s - loss: 0.6739 - acc: 0.580 - ETA: 5s - loss: 0.6739 - acc: 0.580 - ETA: 5s - loss: 0.6736 - acc: 0.580 - ETA: 4s - loss: 0.6737 - acc: 0.580 - ETA: 4s - loss: 0.6734 - acc: 0.581 - ETA: 3s - loss: 0.6731 - acc: 0.581 - ETA: 2s - loss: 0.6729 - acc: 0.581 - ETA: 2s - loss: 0.6728 - acc: 0.581 - ETA: 1s - loss: 0.6729 - acc: 0.580 - ETA: 1s - loss: 0.6731 - acc: 0.580 - ETA: 0s - loss: 0.6730 - acc: 0.580 - 149s 31ms/step - loss: 0.6726 - acc: 0.5815 - val_loss: 0.8916 - val_acc: 0.5305\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4140/4879 [========================>.....] - ETA: 2:29 - loss: 0.6339 - acc: 0.650 - ETA: 2:27 - loss: 0.6558 - acc: 0.650 - ETA: 2:25 - loss: 0.6231 - acc: 0.650 - ETA: 2:24 - loss: 0.6379 - acc: 0.700 - ETA: 2:23 - loss: 0.6327 - acc: 0.700 - ETA: 2:22 - loss: 0.6338 - acc: 0.691 - ETA: 2:22 - loss: 0.6293 - acc: 0.700 - ETA: 2:21 - loss: 0.6525 - acc: 0.668 - ETA: 2:20 - loss: 0.6360 - acc: 0.677 - ETA: 2:19 - loss: 0.6455 - acc: 0.665 - ETA: 2:19 - loss: 0.6562 - acc: 0.654 - ETA: 2:18 - loss: 0.6613 - acc: 0.650 - ETA: 2:18 - loss: 0.6655 - acc: 0.642 - ETA: 2:17 - loss: 0.6729 - acc: 0.635 - ETA: 2:16 - loss: 0.6713 - acc: 0.636 - ETA: 2:16 - loss: 0.6658 - acc: 0.643 - ETA: 2:15 - loss: 0.6655 - acc: 0.647 - ETA: 2:14 - loss: 0.6697 - acc: 0.638 - ETA: 2:14 - loss: 0.6708 - acc: 0.631 - ETA: 2:13 - loss: 0.6758 - acc: 0.630 - ETA: 2:13 - loss: 0.6731 - acc: 0.633 - ETA: 2:12 - loss: 0.6676 - acc: 0.640 - ETA: 2:11 - loss: 0.6684 - acc: 0.639 - ETA: 2:11 - loss: 0.6651 - acc: 0.643 - ETA: 2:10 - loss: 0.6663 - acc: 0.640 - ETA: 2:10 - loss: 0.6660 - acc: 0.640 - ETA: 2:09 - loss: 0.6690 - acc: 0.633 - ETA: 2:08 - loss: 0.6672 - acc: 0.635 - ETA: 2:08 - loss: 0.6635 - acc: 0.641 - ETA: 2:07 - loss: 0.6642 - acc: 0.640 - ETA: 2:07 - loss: 0.6621 - acc: 0.640 - ETA: 2:06 - loss: 0.6649 - acc: 0.634 - ETA: 2:05 - loss: 0.6649 - acc: 0.631 - ETA: 2:05 - loss: 0.6630 - acc: 0.633 - ETA: 2:05 - loss: 0.6644 - acc: 0.634 - ETA: 2:04 - loss: 0.6596 - acc: 0.637 - ETA: 2:03 - loss: 0.6598 - acc: 0.637 - ETA: 2:03 - loss: 0.6561 - acc: 0.642 - ETA: 2:02 - loss: 0.6556 - acc: 0.639 - ETA: 2:01 - loss: 0.6584 - acc: 0.640 - ETA: 2:01 - loss: 0.6550 - acc: 0.646 - ETA: 2:00 - loss: 0.6573 - acc: 0.642 - ETA: 2:00 - loss: 0.6551 - acc: 0.645 - ETA: 1:59 - loss: 0.6542 - acc: 0.645 - ETA: 1:59 - loss: 0.6522 - acc: 0.648 - ETA: 1:58 - loss: 0.6515 - acc: 0.651 - ETA: 1:58 - loss: 0.6481 - acc: 0.654 - ETA: 1:57 - loss: 0.6489 - acc: 0.653 - ETA: 1:56 - loss: 0.6458 - acc: 0.656 - ETA: 1:56 - loss: 0.6452 - acc: 0.657 - ETA: 1:55 - loss: 0.6450 - acc: 0.658 - ETA: 1:55 - loss: 0.6439 - acc: 0.659 - ETA: 1:54 - loss: 0.6443 - acc: 0.657 - ETA: 1:53 - loss: 0.6448 - acc: 0.655 - ETA: 1:53 - loss: 0.6414 - acc: 0.658 - ETA: 1:52 - loss: 0.6401 - acc: 0.659 - ETA: 1:52 - loss: 0.6383 - acc: 0.661 - ETA: 1:51 - loss: 0.6388 - acc: 0.660 - ETA: 1:50 - loss: 0.6369 - acc: 0.662 - ETA: 1:50 - loss: 0.6341 - acc: 0.665 - ETA: 1:49 - loss: 0.6360 - acc: 0.663 - ETA: 1:49 - loss: 0.6356 - acc: 0.662 - ETA: 1:48 - loss: 0.6358 - acc: 0.663 - ETA: 1:48 - loss: 0.6359 - acc: 0.663 - ETA: 1:47 - loss: 0.6361 - acc: 0.662 - ETA: 1:46 - loss: 0.6345 - acc: 0.662 - ETA: 1:46 - loss: 0.6316 - acc: 0.664 - ETA: 1:45 - loss: 0.6316 - acc: 0.663 - ETA: 1:45 - loss: 0.6329 - acc: 0.660 - ETA: 1:44 - loss: 0.6336 - acc: 0.662 - ETA: 1:43 - loss: 0.6323 - acc: 0.663 - ETA: 1:43 - loss: 0.6292 - acc: 0.666 - ETA: 1:42 - loss: 0.6313 - acc: 0.664 - ETA: 1:42 - loss: 0.6310 - acc: 0.665 - ETA: 1:41 - loss: 0.6316 - acc: 0.664 - ETA: 1:40 - loss: 0.6332 - acc: 0.662 - ETA: 1:40 - loss: 0.6322 - acc: 0.663 - ETA: 1:39 - loss: 0.6322 - acc: 0.662 - ETA: 1:39 - loss: 0.6327 - acc: 0.662 - ETA: 1:38 - loss: 0.6323 - acc: 0.663 - ETA: 1:37 - loss: 0.6326 - acc: 0.663 - ETA: 1:37 - loss: 0.6322 - acc: 0.664 - ETA: 1:36 - loss: 0.6339 - acc: 0.661 - ETA: 1:36 - loss: 0.6336 - acc: 0.661 - ETA: 1:35 - loss: 0.6333 - acc: 0.661 - ETA: 1:34 - loss: 0.6330 - acc: 0.662 - ETA: 1:34 - loss: 0.6331 - acc: 0.660 - ETA: 1:33 - loss: 0.6323 - acc: 0.661 - ETA: 1:33 - loss: 0.6325 - acc: 0.661 - ETA: 1:32 - loss: 0.6325 - acc: 0.662 - ETA: 1:31 - loss: 0.6320 - acc: 0.663 - ETA: 1:31 - loss: 0.6338 - acc: 0.661 - ETA: 1:30 - loss: 0.6331 - acc: 0.661 - ETA: 1:30 - loss: 0.6323 - acc: 0.662 - ETA: 1:29 - loss: 0.6324 - acc: 0.663 - ETA: 1:28 - loss: 0.6306 - acc: 0.664 - ETA: 1:28 - loss: 0.6295 - acc: 0.667 - ETA: 1:27 - loss: 0.6292 - acc: 0.667 - ETA: 1:27 - loss: 0.6277 - acc: 0.669 - ETA: 1:26 - loss: 0.6263 - acc: 0.670 - ETA: 1:25 - loss: 0.6251 - acc: 0.671 - ETA: 1:25 - loss: 0.6248 - acc: 0.671 - ETA: 1:24 - loss: 0.6248 - acc: 0.670 - ETA: 1:24 - loss: 0.6239 - acc: 0.671 - ETA: 1:23 - loss: 0.6215 - acc: 0.673 - ETA: 1:22 - loss: 0.6215 - acc: 0.673 - ETA: 1:22 - loss: 0.6227 - acc: 0.671 - ETA: 1:21 - loss: 0.6237 - acc: 0.670 - ETA: 1:21 - loss: 0.6223 - acc: 0.671 - ETA: 1:20 - loss: 0.6206 - acc: 0.673 - ETA: 1:19 - loss: 0.6196 - acc: 0.673 - ETA: 1:19 - loss: 0.6195 - acc: 0.673 - ETA: 1:18 - loss: 0.6188 - acc: 0.673 - ETA: 1:18 - loss: 0.6204 - acc: 0.672 - ETA: 1:17 - loss: 0.6204 - acc: 0.672 - ETA: 1:16 - loss: 0.6200 - acc: 0.672 - ETA: 1:16 - loss: 0.6184 - acc: 0.674 - ETA: 1:15 - loss: 0.6168 - acc: 0.675 - ETA: 1:15 - loss: 0.6156 - acc: 0.676 - ETA: 1:14 - loss: 0.6168 - acc: 0.676 - ETA: 1:13 - loss: 0.6165 - acc: 0.676 - ETA: 1:13 - loss: 0.6172 - acc: 0.675 - ETA: 1:12 - loss: 0.6170 - acc: 0.676 - ETA: 1:12 - loss: 0.6167 - acc: 0.675 - ETA: 1:11 - loss: 0.6173 - acc: 0.676 - ETA: 1:10 - loss: 0.6157 - acc: 0.677 - ETA: 1:10 - loss: 0.6169 - acc: 0.676 - ETA: 1:09 - loss: 0.6175 - acc: 0.676 - ETA: 1:09 - loss: 0.6160 - acc: 0.677 - ETA: 1:08 - loss: 0.6165 - acc: 0.676 - ETA: 1:07 - loss: 0.6163 - acc: 0.677 - ETA: 1:07 - loss: 0.6164 - acc: 0.678 - ETA: 1:06 - loss: 0.6165 - acc: 0.677 - ETA: 1:06 - loss: 0.6168 - acc: 0.676 - ETA: 1:05 - loss: 0.6167 - acc: 0.676 - ETA: 1:04 - loss: 0.6169 - acc: 0.676 - ETA: 1:04 - loss: 0.6163 - acc: 0.676 - ETA: 1:03 - loss: 0.6183 - acc: 0.674 - ETA: 1:03 - loss: 0.6188 - acc: 0.673 - ETA: 1:02 - loss: 0.6205 - acc: 0.671 - ETA: 1:01 - loss: 0.6201 - acc: 0.672 - ETA: 1:01 - loss: 0.6196 - acc: 0.672 - ETA: 1:00 - loss: 0.6203 - acc: 0.671 - ETA: 1:00 - loss: 0.6208 - acc: 0.669 - ETA: 59s - loss: 0.6205 - acc: 0.669 - ETA: 58s - loss: 0.6206 - acc: 0.66 - ETA: 58s - loss: 0.6208 - acc: 0.66 - ETA: 57s - loss: 0.6202 - acc: 0.66 - ETA: 57s - loss: 0.6201 - acc: 0.66 - ETA: 56s - loss: 0.6206 - acc: 0.66 - ETA: 55s - loss: 0.6205 - acc: 0.66 - ETA: 55s - loss: 0.6198 - acc: 0.66 - ETA: 54s - loss: 0.6195 - acc: 0.66 - ETA: 54s - loss: 0.6197 - acc: 0.66 - ETA: 53s - loss: 0.6189 - acc: 0.66 - ETA: 52s - loss: 0.6187 - acc: 0.66 - ETA: 52s - loss: 0.6182 - acc: 0.66 - ETA: 51s - loss: 0.6176 - acc: 0.66 - ETA: 51s - loss: 0.6170 - acc: 0.67 - ETA: 50s - loss: 0.6161 - acc: 0.67 - ETA: 49s - loss: 0.6163 - acc: 0.67 - ETA: 49s - loss: 0.6156 - acc: 0.67 - ETA: 48s - loss: 0.6163 - acc: 0.67 - ETA: 48s - loss: 0.6157 - acc: 0.67 - ETA: 47s - loss: 0.6161 - acc: 0.67 - ETA: 46s - loss: 0.6152 - acc: 0.67 - ETA: 46s - loss: 0.6156 - acc: 0.67 - ETA: 45s - loss: 0.6153 - acc: 0.67 - ETA: 45s - loss: 0.6147 - acc: 0.67 - ETA: 44s - loss: 0.6146 - acc: 0.67 - ETA: 43s - loss: 0.6144 - acc: 0.67 - ETA: 43s - loss: 0.6143 - acc: 0.67 - ETA: 42s - loss: 0.6138 - acc: 0.67 - ETA: 42s - loss: 0.6135 - acc: 0.67 - ETA: 41s - loss: 0.6135 - acc: 0.67 - ETA: 40s - loss: 0.6137 - acc: 0.66 - ETA: 40s - loss: 0.6133 - acc: 0.67 - ETA: 39s - loss: 0.6139 - acc: 0.66 - ETA: 39s - loss: 0.6135 - acc: 0.66 - ETA: 38s - loss: 0.6127 - acc: 0.67 - ETA: 37s - loss: 0.6128 - acc: 0.66 - ETA: 37s - loss: 0.6117 - acc: 0.67 - ETA: 36s - loss: 0.6111 - acc: 0.67 - ETA: 36s - loss: 0.6126 - acc: 0.66 - ETA: 35s - loss: 0.6119 - acc: 0.67 - ETA: 34s - loss: 0.6106 - acc: 0.67 - ETA: 34s - loss: 0.6100 - acc: 0.67 - ETA: 33s - loss: 0.6091 - acc: 0.67 - ETA: 33s - loss: 0.6094 - acc: 0.67 - ETA: 32s - loss: 0.6093 - acc: 0.67 - ETA: 31s - loss: 0.6087 - acc: 0.67 - ETA: 31s - loss: 0.6078 - acc: 0.67 - ETA: 30s - loss: 0.6075 - acc: 0.67 - ETA: 30s - loss: 0.6072 - acc: 0.67 - ETA: 29s - loss: 0.6070 - acc: 0.67 - ETA: 28s - loss: 0.6063 - acc: 0.67 - ETA: 28s - loss: 0.6058 - acc: 0.67 - ETA: 27s - loss: 0.6051 - acc: 0.67 - ETA: 27s - loss: 0.6046 - acc: 0.67 - ETA: 26s - loss: 0.6041 - acc: 0.67 - ETA: 25s - loss: 0.6044 - acc: 0.67 - ETA: 25s - loss: 0.6034 - acc: 0.67 - ETA: 24s - loss: 0.6036 - acc: 0.67 - ETA: 24s - loss: 0.6025 - acc: 0.67 - ETA: 23s - loss: 0.6037 - acc: 0.67 - ETA: 22s - loss: 0.6039 - acc: 0.67 - ETA: 22s - loss: 0.6037 - acc: 0.6768"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4879/4879 [==============================] - ETA: 21s - loss: 0.6031 - acc: 0.67 - ETA: 21s - loss: 0.6037 - acc: 0.67 - ETA: 20s - loss: 0.6041 - acc: 0.67 - ETA: 19s - loss: 0.6037 - acc: 0.67 - ETA: 19s - loss: 0.6039 - acc: 0.67 - ETA: 18s - loss: 0.6039 - acc: 0.67 - ETA: 17s - loss: 0.6038 - acc: 0.67 - ETA: 17s - loss: 0.6037 - acc: 0.67 - ETA: 16s - loss: 0.6029 - acc: 0.67 - ETA: 16s - loss: 0.6020 - acc: 0.67 - ETA: 15s - loss: 0.6025 - acc: 0.67 - ETA: 14s - loss: 0.6022 - acc: 0.67 - ETA: 14s - loss: 0.6021 - acc: 0.67 - ETA: 13s - loss: 0.6024 - acc: 0.67 - ETA: 13s - loss: 0.6020 - acc: 0.67 - ETA: 12s - loss: 0.6020 - acc: 0.67 - ETA: 11s - loss: 0.6016 - acc: 0.67 - ETA: 11s - loss: 0.6013 - acc: 0.67 - ETA: 10s - loss: 0.6012 - acc: 0.67 - ETA: 10s - loss: 0.6012 - acc: 0.67 - ETA: 9s - loss: 0.6008 - acc: 0.6774 - ETA: 8s - loss: 0.6015 - acc: 0.677 - ETA: 8s - loss: 0.6006 - acc: 0.678 - ETA: 7s - loss: 0.5999 - acc: 0.678 - ETA: 7s - loss: 0.5992 - acc: 0.679 - ETA: 6s - loss: 0.5992 - acc: 0.679 - ETA: 5s - loss: 0.5989 - acc: 0.678 - ETA: 5s - loss: 0.5985 - acc: 0.678 - ETA: 4s - loss: 0.5981 - acc: 0.679 - ETA: 4s - loss: 0.5984 - acc: 0.678 - ETA: 3s - loss: 0.5980 - acc: 0.679 - ETA: 2s - loss: 0.5978 - acc: 0.679 - ETA: 2s - loss: 0.5977 - acc: 0.679 - ETA: 1s - loss: 0.5972 - acc: 0.680 - ETA: 1s - loss: 0.5975 - acc: 0.679 - ETA: 0s - loss: 0.5970 - acc: 0.680 - 150s 31ms/step - loss: 0.5977 - acc: 0.6799 - val_loss: 0.8373 - val_acc: 0.5177\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4140/4879 [========================>.....] - ETA: 2:26 - loss: 0.4912 - acc: 0.800 - ETA: 2:25 - loss: 0.4617 - acc: 0.825 - ETA: 2:24 - loss: 0.5022 - acc: 0.766 - ETA: 2:24 - loss: 0.5170 - acc: 0.750 - ETA: 2:24 - loss: 0.5441 - acc: 0.710 - ETA: 2:24 - loss: 0.5300 - acc: 0.725 - ETA: 2:23 - loss: 0.5247 - acc: 0.735 - ETA: 2:22 - loss: 0.5214 - acc: 0.743 - ETA: 2:21 - loss: 0.5063 - acc: 0.755 - ETA: 2:21 - loss: 0.5116 - acc: 0.745 - ETA: 2:20 - loss: 0.5080 - acc: 0.754 - ETA: 2:19 - loss: 0.4918 - acc: 0.770 - ETA: 2:19 - loss: 0.4822 - acc: 0.776 - ETA: 2:18 - loss: 0.4815 - acc: 0.785 - ETA: 2:18 - loss: 0.4722 - acc: 0.796 - ETA: 2:17 - loss: 0.4802 - acc: 0.790 - ETA: 2:17 - loss: 0.4705 - acc: 0.800 - ETA: 2:16 - loss: 0.4588 - acc: 0.805 - ETA: 2:15 - loss: 0.4660 - acc: 0.797 - ETA: 2:15 - loss: 0.4766 - acc: 0.790 - ETA: 2:14 - loss: 0.4736 - acc: 0.792 - ETA: 2:13 - loss: 0.4674 - acc: 0.793 - ETA: 2:13 - loss: 0.4684 - acc: 0.789 - ETA: 2:12 - loss: 0.4727 - acc: 0.783 - ETA: 2:12 - loss: 0.4690 - acc: 0.788 - ETA: 2:11 - loss: 0.4700 - acc: 0.788 - ETA: 2:10 - loss: 0.4642 - acc: 0.790 - ETA: 2:10 - loss: 0.4602 - acc: 0.792 - ETA: 2:09 - loss: 0.4605 - acc: 0.789 - ETA: 2:08 - loss: 0.4634 - acc: 0.785 - ETA: 2:08 - loss: 0.4664 - acc: 0.780 - ETA: 2:07 - loss: 0.4664 - acc: 0.779 - ETA: 2:06 - loss: 0.4617 - acc: 0.784 - ETA: 2:06 - loss: 0.4676 - acc: 0.777 - ETA: 2:05 - loss: 0.4715 - acc: 0.775 - ETA: 2:05 - loss: 0.4734 - acc: 0.772 - ETA: 2:04 - loss: 0.4716 - acc: 0.774 - ETA: 2:04 - loss: 0.4715 - acc: 0.773 - ETA: 2:03 - loss: 0.4765 - acc: 0.770 - ETA: 2:02 - loss: 0.4757 - acc: 0.768 - ETA: 2:02 - loss: 0.4713 - acc: 0.772 - ETA: 2:01 - loss: 0.4763 - acc: 0.767 - ETA: 2:00 - loss: 0.4755 - acc: 0.767 - ETA: 2:00 - loss: 0.4740 - acc: 0.770 - ETA: 1:59 - loss: 0.4716 - acc: 0.772 - ETA: 1:59 - loss: 0.4700 - acc: 0.772 - ETA: 1:58 - loss: 0.4650 - acc: 0.776 - ETA: 1:57 - loss: 0.4714 - acc: 0.772 - ETA: 1:57 - loss: 0.4685 - acc: 0.775 - ETA: 1:56 - loss: 0.4688 - acc: 0.776 - ETA: 1:56 - loss: 0.4703 - acc: 0.776 - ETA: 1:55 - loss: 0.4723 - acc: 0.775 - ETA: 1:54 - loss: 0.4709 - acc: 0.776 - ETA: 1:54 - loss: 0.4689 - acc: 0.776 - ETA: 1:53 - loss: 0.4684 - acc: 0.777 - ETA: 1:53 - loss: 0.4656 - acc: 0.778 - ETA: 1:52 - loss: 0.4651 - acc: 0.779 - ETA: 1:51 - loss: 0.4664 - acc: 0.778 - ETA: 1:51 - loss: 0.4672 - acc: 0.778 - ETA: 1:50 - loss: 0.4659 - acc: 0.778 - ETA: 1:50 - loss: 0.4653 - acc: 0.779 - ETA: 1:49 - loss: 0.4680 - acc: 0.778 - ETA: 1:48 - loss: 0.4693 - acc: 0.777 - ETA: 1:48 - loss: 0.4716 - acc: 0.777 - ETA: 1:47 - loss: 0.4720 - acc: 0.777 - ETA: 1:47 - loss: 0.4745 - acc: 0.777 - ETA: 1:46 - loss: 0.4712 - acc: 0.779 - ETA: 1:45 - loss: 0.4714 - acc: 0.779 - ETA: 1:45 - loss: 0.4707 - acc: 0.779 - ETA: 1:44 - loss: 0.4687 - acc: 0.780 - ETA: 1:44 - loss: 0.4686 - acc: 0.779 - ETA: 1:43 - loss: 0.4681 - acc: 0.778 - ETA: 1:42 - loss: 0.4663 - acc: 0.778 - ETA: 1:42 - loss: 0.4643 - acc: 0.780 - ETA: 1:41 - loss: 0.4653 - acc: 0.779 - ETA: 1:40 - loss: 0.4634 - acc: 0.780 - ETA: 1:40 - loss: 0.4639 - acc: 0.779 - ETA: 1:39 - loss: 0.4636 - acc: 0.780 - ETA: 1:39 - loss: 0.4638 - acc: 0.779 - ETA: 1:38 - loss: 0.4622 - acc: 0.781 - ETA: 1:37 - loss: 0.4605 - acc: 0.782 - ETA: 1:37 - loss: 0.4600 - acc: 0.782 - ETA: 1:36 - loss: 0.4569 - acc: 0.783 - ETA: 1:36 - loss: 0.4573 - acc: 0.782 - ETA: 1:35 - loss: 0.4578 - acc: 0.782 - ETA: 1:34 - loss: 0.4554 - acc: 0.784 - ETA: 1:34 - loss: 0.4526 - acc: 0.786 - ETA: 1:33 - loss: 0.4526 - acc: 0.786 - ETA: 1:33 - loss: 0.4524 - acc: 0.786 - ETA: 1:32 - loss: 0.4505 - acc: 0.787 - ETA: 1:31 - loss: 0.4510 - acc: 0.785 - ETA: 1:31 - loss: 0.4509 - acc: 0.785 - ETA: 1:30 - loss: 0.4500 - acc: 0.786 - ETA: 1:30 - loss: 0.4510 - acc: 0.786 - ETA: 1:29 - loss: 0.4510 - acc: 0.786 - ETA: 1:28 - loss: 0.4505 - acc: 0.786 - ETA: 1:28 - loss: 0.4500 - acc: 0.786 - ETA: 1:27 - loss: 0.4500 - acc: 0.785 - ETA: 1:27 - loss: 0.4516 - acc: 0.784 - ETA: 1:26 - loss: 0.4499 - acc: 0.786 - ETA: 1:25 - loss: 0.4494 - acc: 0.786 - ETA: 1:25 - loss: 0.4487 - acc: 0.786 - ETA: 1:24 - loss: 0.4494 - acc: 0.786 - ETA: 1:24 - loss: 0.4484 - acc: 0.787 - ETA: 1:23 - loss: 0.4480 - acc: 0.787 - ETA: 1:22 - loss: 0.4495 - acc: 0.786 - ETA: 1:22 - loss: 0.4500 - acc: 0.786 - ETA: 1:21 - loss: 0.4492 - acc: 0.786 - ETA: 1:21 - loss: 0.4487 - acc: 0.787 - ETA: 1:20 - loss: 0.4489 - acc: 0.787 - ETA: 1:19 - loss: 0.4488 - acc: 0.788 - ETA: 1:19 - loss: 0.4481 - acc: 0.788 - ETA: 1:18 - loss: 0.4474 - acc: 0.789 - ETA: 1:18 - loss: 0.4481 - acc: 0.789 - ETA: 1:17 - loss: 0.4478 - acc: 0.788 - ETA: 1:16 - loss: 0.4472 - acc: 0.788 - ETA: 1:16 - loss: 0.4486 - acc: 0.787 - ETA: 1:15 - loss: 0.4470 - acc: 0.789 - ETA: 1:15 - loss: 0.4478 - acc: 0.787 - ETA: 1:14 - loss: 0.4488 - acc: 0.787 - ETA: 1:13 - loss: 0.4486 - acc: 0.786 - ETA: 1:13 - loss: 0.4480 - acc: 0.786 - ETA: 1:12 - loss: 0.4488 - acc: 0.786 - ETA: 1:12 - loss: 0.4535 - acc: 0.783 - ETA: 1:11 - loss: 0.4538 - acc: 0.783 - ETA: 1:10 - loss: 0.4564 - acc: 0.781 - ETA: 1:10 - loss: 0.4540 - acc: 0.783 - ETA: 1:09 - loss: 0.4535 - acc: 0.782 - ETA: 1:09 - loss: 0.4529 - acc: 0.782 - ETA: 1:08 - loss: 0.4515 - acc: 0.783 - ETA: 1:07 - loss: 0.4521 - acc: 0.783 - ETA: 1:07 - loss: 0.4518 - acc: 0.783 - ETA: 1:06 - loss: 0.4521 - acc: 0.782 - ETA: 1:06 - loss: 0.4530 - acc: 0.782 - ETA: 1:05 - loss: 0.4527 - acc: 0.782 - ETA: 1:04 - loss: 0.4528 - acc: 0.782 - ETA: 1:04 - loss: 0.4530 - acc: 0.782 - ETA: 1:03 - loss: 0.4536 - acc: 0.782 - ETA: 1:03 - loss: 0.4540 - acc: 0.781 - ETA: 1:02 - loss: 0.4539 - acc: 0.781 - ETA: 1:01 - loss: 0.4531 - acc: 0.782 - ETA: 1:01 - loss: 0.4544 - acc: 0.781 - ETA: 1:00 - loss: 0.4536 - acc: 0.781 - ETA: 1:00 - loss: 0.4538 - acc: 0.781 - ETA: 59s - loss: 0.4538 - acc: 0.781 - ETA: 58s - loss: 0.4530 - acc: 0.78 - ETA: 58s - loss: 0.4533 - acc: 0.78 - ETA: 57s - loss: 0.4527 - acc: 0.78 - ETA: 57s - loss: 0.4521 - acc: 0.78 - ETA: 56s - loss: 0.4538 - acc: 0.78 - ETA: 55s - loss: 0.4529 - acc: 0.78 - ETA: 55s - loss: 0.4524 - acc: 0.78 - ETA: 54s - loss: 0.4524 - acc: 0.78 - ETA: 54s - loss: 0.4537 - acc: 0.78 - ETA: 53s - loss: 0.4539 - acc: 0.78 - ETA: 52s - loss: 0.4544 - acc: 0.78 - ETA: 52s - loss: 0.4560 - acc: 0.78 - ETA: 51s - loss: 0.4558 - acc: 0.78 - ETA: 51s - loss: 0.4555 - acc: 0.78 - ETA: 50s - loss: 0.4556 - acc: 0.78 - ETA: 49s - loss: 0.4560 - acc: 0.78 - ETA: 49s - loss: 0.4565 - acc: 0.78 - ETA: 48s - loss: 0.4567 - acc: 0.78 - ETA: 48s - loss: 0.4570 - acc: 0.77 - ETA: 47s - loss: 0.4567 - acc: 0.77 - ETA: 46s - loss: 0.4551 - acc: 0.78 - ETA: 46s - loss: 0.4544 - acc: 0.78 - ETA: 45s - loss: 0.4550 - acc: 0.78 - ETA: 45s - loss: 0.4546 - acc: 0.78 - ETA: 44s - loss: 0.4549 - acc: 0.78 - ETA: 43s - loss: 0.4551 - acc: 0.78 - ETA: 43s - loss: 0.4545 - acc: 0.78 - ETA: 42s - loss: 0.4540 - acc: 0.78 - ETA: 42s - loss: 0.4539 - acc: 0.78 - ETA: 41s - loss: 0.4544 - acc: 0.78 - ETA: 40s - loss: 0.4561 - acc: 0.78 - ETA: 40s - loss: 0.4551 - acc: 0.78 - ETA: 39s - loss: 0.4551 - acc: 0.78 - ETA: 39s - loss: 0.4563 - acc: 0.78 - ETA: 38s - loss: 0.4585 - acc: 0.78 - ETA: 37s - loss: 0.4589 - acc: 0.78 - ETA: 37s - loss: 0.4587 - acc: 0.78 - ETA: 36s - loss: 0.4577 - acc: 0.78 - ETA: 36s - loss: 0.4582 - acc: 0.78 - ETA: 35s - loss: 0.4574 - acc: 0.78 - ETA: 34s - loss: 0.4575 - acc: 0.78 - ETA: 34s - loss: 0.4575 - acc: 0.78 - ETA: 33s - loss: 0.4589 - acc: 0.78 - ETA: 33s - loss: 0.4582 - acc: 0.78 - ETA: 32s - loss: 0.4578 - acc: 0.78 - ETA: 31s - loss: 0.4573 - acc: 0.78 - ETA: 31s - loss: 0.4572 - acc: 0.78 - ETA: 30s - loss: 0.4586 - acc: 0.78 - ETA: 30s - loss: 0.4578 - acc: 0.78 - ETA: 29s - loss: 0.4569 - acc: 0.78 - ETA: 28s - loss: 0.4560 - acc: 0.78 - ETA: 28s - loss: 0.4565 - acc: 0.78 - ETA: 27s - loss: 0.4576 - acc: 0.78 - ETA: 27s - loss: 0.4588 - acc: 0.78 - ETA: 26s - loss: 0.4591 - acc: 0.78 - ETA: 25s - loss: 0.4598 - acc: 0.78 - ETA: 25s - loss: 0.4608 - acc: 0.78 - ETA: 24s - loss: 0.4610 - acc: 0.77 - ETA: 24s - loss: 0.4612 - acc: 0.77 - ETA: 23s - loss: 0.4608 - acc: 0.78 - ETA: 22s - loss: 0.4608 - acc: 0.77 - ETA: 22s - loss: 0.4609 - acc: 0.7797"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4879/4879 [==============================] - ETA: 21s - loss: 0.4605 - acc: 0.78 - ETA: 20s - loss: 0.4610 - acc: 0.77 - ETA: 20s - loss: 0.4612 - acc: 0.77 - ETA: 19s - loss: 0.4610 - acc: 0.77 - ETA: 19s - loss: 0.4607 - acc: 0.77 - ETA: 18s - loss: 0.4607 - acc: 0.77 - ETA: 17s - loss: 0.4598 - acc: 0.77 - ETA: 17s - loss: 0.4597 - acc: 0.78 - ETA: 16s - loss: 0.4590 - acc: 0.78 - ETA: 16s - loss: 0.4586 - acc: 0.78 - ETA: 15s - loss: 0.4581 - acc: 0.78 - ETA: 14s - loss: 0.4572 - acc: 0.78 - ETA: 14s - loss: 0.4577 - acc: 0.78 - ETA: 13s - loss: 0.4584 - acc: 0.77 - ETA: 13s - loss: 0.4577 - acc: 0.78 - ETA: 12s - loss: 0.4579 - acc: 0.77 - ETA: 11s - loss: 0.4573 - acc: 0.78 - ETA: 11s - loss: 0.4565 - acc: 0.78 - ETA: 10s - loss: 0.4566 - acc: 0.78 - ETA: 10s - loss: 0.4571 - acc: 0.78 - ETA: 9s - loss: 0.4572 - acc: 0.7811 - ETA: 8s - loss: 0.4568 - acc: 0.781 - ETA: 8s - loss: 0.4568 - acc: 0.781 - ETA: 7s - loss: 0.4573 - acc: 0.781 - ETA: 7s - loss: 0.4578 - acc: 0.781 - ETA: 6s - loss: 0.4570 - acc: 0.782 - ETA: 5s - loss: 0.4576 - acc: 0.781 - ETA: 5s - loss: 0.4588 - acc: 0.781 - ETA: 4s - loss: 0.4594 - acc: 0.780 - ETA: 4s - loss: 0.4592 - acc: 0.781 - ETA: 3s - loss: 0.4587 - acc: 0.781 - ETA: 2s - loss: 0.4589 - acc: 0.781 - ETA: 2s - loss: 0.4594 - acc: 0.780 - ETA: 1s - loss: 0.4593 - acc: 0.780 - ETA: 1s - loss: 0.4588 - acc: 0.780 - ETA: 0s - loss: 0.4585 - acc: 0.780 - 150s 31ms/step - loss: 0.4582 - acc: 0.7807 - val_loss: 1.0536 - val_acc: 0.5241\n",
      "Train on 2793 samples, validate on 311 samples\n",
      "Epoch 1/4\n",
      "2793/2793 [==============================] - ETA: 1:24 - loss: 1.0134 - acc: 0.550 - ETA: 1:23 - loss: 0.9333 - acc: 0.575 - ETA: 1:22 - loss: 0.9410 - acc: 0.566 - ETA: 1:22 - loss: 0.9666 - acc: 0.525 - ETA: 1:21 - loss: 0.9423 - acc: 0.520 - ETA: 1:20 - loss: 0.9526 - acc: 0.508 - ETA: 1:20 - loss: 0.9234 - acc: 0.507 - ETA: 1:19 - loss: 0.9012 - acc: 0.500 - ETA: 1:18 - loss: 0.8797 - acc: 0.505 - ETA: 1:18 - loss: 0.8661 - acc: 0.510 - ETA: 1:17 - loss: 0.8468 - acc: 0.527 - ETA: 1:17 - loss: 0.8352 - acc: 0.520 - ETA: 1:16 - loss: 0.8250 - acc: 0.526 - ETA: 1:15 - loss: 0.8138 - acc: 0.539 - ETA: 1:15 - loss: 0.8120 - acc: 0.526 - ETA: 1:14 - loss: 0.8076 - acc: 0.518 - ETA: 1:14 - loss: 0.8008 - acc: 0.517 - ETA: 1:13 - loss: 0.7953 - acc: 0.513 - ETA: 1:12 - loss: 0.7904 - acc: 0.510 - ETA: 1:12 - loss: 0.7871 - acc: 0.507 - ETA: 1:11 - loss: 0.7837 - acc: 0.504 - ETA: 1:10 - loss: 0.7771 - acc: 0.511 - ETA: 1:10 - loss: 0.7773 - acc: 0.504 - ETA: 1:09 - loss: 0.7725 - acc: 0.510 - ETA: 1:09 - loss: 0.7701 - acc: 0.508 - ETA: 1:08 - loss: 0.7659 - acc: 0.513 - ETA: 1:08 - loss: 0.7622 - acc: 0.520 - ETA: 1:07 - loss: 0.7593 - acc: 0.521 - ETA: 1:06 - loss: 0.7572 - acc: 0.515 - ETA: 1:06 - loss: 0.7545 - acc: 0.516 - ETA: 1:05 - loss: 0.7518 - acc: 0.521 - ETA: 1:04 - loss: 0.7490 - acc: 0.523 - ETA: 1:04 - loss: 0.7488 - acc: 0.521 - ETA: 1:03 - loss: 0.7489 - acc: 0.514 - ETA: 1:03 - loss: 0.7482 - acc: 0.512 - ETA: 1:02 - loss: 0.7466 - acc: 0.513 - ETA: 1:02 - loss: 0.7454 - acc: 0.512 - ETA: 1:01 - loss: 0.7444 - acc: 0.509 - ETA: 1:00 - loss: 0.7430 - acc: 0.510 - ETA: 1:00 - loss: 0.7410 - acc: 0.513 - ETA: 59s - loss: 0.7404 - acc: 0.509 - ETA: 58s - loss: 0.7400 - acc: 0.50 - ETA: 58s - loss: 0.7395 - acc: 0.50 - ETA: 57s - loss: 0.7393 - acc: 0.50 - ETA: 57s - loss: 0.7374 - acc: 0.50 - ETA: 56s - loss: 0.7356 - acc: 0.50 - ETA: 55s - loss: 0.7363 - acc: 0.50 - ETA: 55s - loss: 0.7350 - acc: 0.50 - ETA: 54s - loss: 0.7359 - acc: 0.50 - ETA: 54s - loss: 0.7339 - acc: 0.50 - ETA: 53s - loss: 0.7345 - acc: 0.50 - ETA: 52s - loss: 0.7331 - acc: 0.50 - ETA: 52s - loss: 0.7339 - acc: 0.50 - ETA: 51s - loss: 0.7329 - acc: 0.50 - ETA: 51s - loss: 0.7323 - acc: 0.50 - ETA: 50s - loss: 0.7321 - acc: 0.50 - ETA: 49s - loss: 0.7319 - acc: 0.50 - ETA: 49s - loss: 0.7312 - acc: 0.50 - ETA: 48s - loss: 0.7304 - acc: 0.50 - ETA: 48s - loss: 0.7288 - acc: 0.51 - ETA: 47s - loss: 0.7272 - acc: 0.51 - ETA: 46s - loss: 0.7275 - acc: 0.51 - ETA: 46s - loss: 0.7272 - acc: 0.51 - ETA: 45s - loss: 0.7258 - acc: 0.51 - ETA: 44s - loss: 0.7266 - acc: 0.51 - ETA: 44s - loss: 0.7273 - acc: 0.51 - ETA: 43s - loss: 0.7271 - acc: 0.51 - ETA: 43s - loss: 0.7265 - acc: 0.51 - ETA: 42s - loss: 0.7260 - acc: 0.51 - ETA: 41s - loss: 0.7255 - acc: 0.51 - ETA: 41s - loss: 0.7255 - acc: 0.51 - ETA: 40s - loss: 0.7258 - acc: 0.50 - ETA: 40s - loss: 0.7252 - acc: 0.50 - ETA: 39s - loss: 0.7252 - acc: 0.50 - ETA: 39s - loss: 0.7243 - acc: 0.50 - ETA: 38s - loss: 0.7241 - acc: 0.50 - ETA: 37s - loss: 0.7237 - acc: 0.50 - ETA: 37s - loss: 0.7232 - acc: 0.50 - ETA: 36s - loss: 0.7232 - acc: 0.50 - ETA: 36s - loss: 0.7229 - acc: 0.50 - ETA: 35s - loss: 0.7225 - acc: 0.50 - ETA: 34s - loss: 0.7222 - acc: 0.50 - ETA: 34s - loss: 0.7220 - acc: 0.50 - ETA: 33s - loss: 0.7217 - acc: 0.50 - ETA: 33s - loss: 0.7210 - acc: 0.50 - ETA: 32s - loss: 0.7212 - acc: 0.50 - ETA: 31s - loss: 0.7210 - acc: 0.50 - ETA: 31s - loss: 0.7208 - acc: 0.50 - ETA: 30s - loss: 0.7206 - acc: 0.50 - ETA: 29s - loss: 0.7204 - acc: 0.50 - ETA: 29s - loss: 0.7200 - acc: 0.50 - ETA: 28s - loss: 0.7195 - acc: 0.50 - ETA: 28s - loss: 0.7193 - acc: 0.50 - ETA: 27s - loss: 0.7191 - acc: 0.50 - ETA: 26s - loss: 0.7188 - acc: 0.50 - ETA: 26s - loss: 0.7184 - acc: 0.50 - ETA: 25s - loss: 0.7183 - acc: 0.50 - ETA: 25s - loss: 0.7184 - acc: 0.50 - ETA: 24s - loss: 0.7182 - acc: 0.50 - ETA: 23s - loss: 0.7178 - acc: 0.50 - ETA: 23s - loss: 0.7179 - acc: 0.50 - ETA: 22s - loss: 0.7176 - acc: 0.50 - ETA: 22s - loss: 0.7175 - acc: 0.50 - ETA: 21s - loss: 0.7174 - acc: 0.50 - ETA: 20s - loss: 0.7170 - acc: 0.50 - ETA: 20s - loss: 0.7170 - acc: 0.50 - ETA: 19s - loss: 0.7169 - acc: 0.50 - ETA: 19s - loss: 0.7167 - acc: 0.50 - ETA: 18s - loss: 0.7162 - acc: 0.50 - ETA: 17s - loss: 0.7162 - acc: 0.50 - ETA: 17s - loss: 0.7156 - acc: 0.50 - ETA: 16s - loss: 0.7154 - acc: 0.50 - ETA: 16s - loss: 0.7155 - acc: 0.50 - ETA: 15s - loss: 0.7157 - acc: 0.50 - ETA: 14s - loss: 0.7160 - acc: 0.50 - ETA: 14s - loss: 0.7157 - acc: 0.50 - ETA: 13s - loss: 0.7155 - acc: 0.50 - ETA: 13s - loss: 0.7155 - acc: 0.50 - ETA: 12s - loss: 0.7157 - acc: 0.50 - ETA: 11s - loss: 0.7155 - acc: 0.50 - ETA: 11s - loss: 0.7157 - acc: 0.50 - ETA: 10s - loss: 0.7160 - acc: 0.50 - ETA: 10s - loss: 0.7153 - acc: 0.50 - ETA: 9s - loss: 0.7151 - acc: 0.5020 - ETA: 8s - loss: 0.7146 - acc: 0.503 - ETA: 8s - loss: 0.7146 - acc: 0.502 - ETA: 7s - loss: 0.7144 - acc: 0.503 - ETA: 7s - loss: 0.7142 - acc: 0.503 - ETA: 6s - loss: 0.7140 - acc: 0.503 - ETA: 5s - loss: 0.7138 - acc: 0.503 - ETA: 5s - loss: 0.7136 - acc: 0.503 - ETA: 4s - loss: 0.7136 - acc: 0.502 - ETA: 4s - loss: 0.7134 - acc: 0.503 - ETA: 3s - loss: 0.7131 - acc: 0.504 - ETA: 2s - loss: 0.7128 - acc: 0.505 - ETA: 2s - loss: 0.7127 - acc: 0.505 - ETA: 1s - loss: 0.7126 - acc: 0.505 - ETA: 0s - loss: 0.7125 - acc: 0.504 - ETA: 0s - loss: 0.7124 - acc: 0.505 - 88s 31ms/step - loss: 0.7123 - acc: 0.5048 - val_loss: 0.6865 - val_acc: 0.5627\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2793/2793 [==============================] - ETA: 1:22 - loss: 0.6845 - acc: 0.500 - ETA: 1:22 - loss: 0.6914 - acc: 0.525 - ETA: 1:21 - loss: 0.6918 - acc: 0.533 - ETA: 1:22 - loss: 0.6885 - acc: 0.537 - ETA: 1:21 - loss: 0.6991 - acc: 0.490 - ETA: 1:21 - loss: 0.6908 - acc: 0.516 - ETA: 1:20 - loss: 0.6872 - acc: 0.521 - ETA: 1:19 - loss: 0.6850 - acc: 0.531 - ETA: 1:19 - loss: 0.6840 - acc: 0.533 - ETA: 1:18 - loss: 0.6845 - acc: 0.535 - ETA: 1:17 - loss: 0.6869 - acc: 0.513 - ETA: 1:17 - loss: 0.6873 - acc: 0.516 - ETA: 1:16 - loss: 0.6872 - acc: 0.519 - ETA: 1:16 - loss: 0.6868 - acc: 0.521 - ETA: 1:15 - loss: 0.6871 - acc: 0.520 - ETA: 1:14 - loss: 0.6893 - acc: 0.512 - ETA: 1:14 - loss: 0.6861 - acc: 0.523 - ETA: 1:13 - loss: 0.6828 - acc: 0.530 - ETA: 1:12 - loss: 0.6834 - acc: 0.531 - ETA: 1:12 - loss: 0.6796 - acc: 0.540 - ETA: 1:11 - loss: 0.6814 - acc: 0.540 - ETA: 1:11 - loss: 0.6834 - acc: 0.540 - ETA: 1:10 - loss: 0.6886 - acc: 0.534 - ETA: 1:09 - loss: 0.6897 - acc: 0.533 - ETA: 1:09 - loss: 0.6912 - acc: 0.530 - ETA: 1:08 - loss: 0.6903 - acc: 0.534 - ETA: 1:08 - loss: 0.6911 - acc: 0.531 - ETA: 1:07 - loss: 0.6908 - acc: 0.533 - ETA: 1:06 - loss: 0.6901 - acc: 0.539 - ETA: 1:06 - loss: 0.6902 - acc: 0.540 - ETA: 1:05 - loss: 0.6884 - acc: 0.543 - ETA: 1:05 - loss: 0.6887 - acc: 0.540 - ETA: 1:04 - loss: 0.6910 - acc: 0.534 - ETA: 1:03 - loss: 0.6887 - acc: 0.539 - ETA: 1:03 - loss: 0.6891 - acc: 0.540 - ETA: 1:02 - loss: 0.6908 - acc: 0.537 - ETA: 1:02 - loss: 0.6914 - acc: 0.535 - ETA: 1:01 - loss: 0.6896 - acc: 0.538 - ETA: 1:00 - loss: 0.6887 - acc: 0.538 - ETA: 1:00 - loss: 0.6885 - acc: 0.537 - ETA: 59s - loss: 0.6888 - acc: 0.537 - ETA: 59s - loss: 0.6889 - acc: 0.53 - ETA: 58s - loss: 0.6893 - acc: 0.53 - ETA: 57s - loss: 0.6888 - acc: 0.53 - ETA: 57s - loss: 0.6881 - acc: 0.53 - ETA: 56s - loss: 0.6880 - acc: 0.53 - ETA: 55s - loss: 0.6873 - acc: 0.54 - ETA: 55s - loss: 0.6878 - acc: 0.53 - ETA: 54s - loss: 0.6874 - acc: 0.53 - ETA: 54s - loss: 0.6871 - acc: 0.53 - ETA: 53s - loss: 0.6864 - acc: 0.53 - ETA: 52s - loss: 0.6864 - acc: 0.54 - ETA: 52s - loss: 0.6864 - acc: 0.54 - ETA: 51s - loss: 0.6862 - acc: 0.54 - ETA: 51s - loss: 0.6855 - acc: 0.54 - ETA: 50s - loss: 0.6849 - acc: 0.55 - ETA: 50s - loss: 0.6851 - acc: 0.54 - ETA: 49s - loss: 0.6865 - acc: 0.54 - ETA: 48s - loss: 0.6873 - acc: 0.54 - ETA: 48s - loss: 0.6868 - acc: 0.54 - ETA: 47s - loss: 0.6870 - acc: 0.54 - ETA: 47s - loss: 0.6869 - acc: 0.54 - ETA: 46s - loss: 0.6870 - acc: 0.54 - ETA: 45s - loss: 0.6865 - acc: 0.54 - ETA: 45s - loss: 0.6868 - acc: 0.54 - ETA: 44s - loss: 0.6874 - acc: 0.54 - ETA: 44s - loss: 0.6875 - acc: 0.54 - ETA: 43s - loss: 0.6870 - acc: 0.54 - ETA: 42s - loss: 0.6871 - acc: 0.54 - ETA: 42s - loss: 0.6857 - acc: 0.54 - ETA: 41s - loss: 0.6860 - acc: 0.54 - ETA: 41s - loss: 0.6853 - acc: 0.54 - ETA: 40s - loss: 0.6849 - acc: 0.54 - ETA: 39s - loss: 0.6848 - acc: 0.55 - ETA: 39s - loss: 0.6845 - acc: 0.55 - ETA: 38s - loss: 0.6841 - acc: 0.55 - ETA: 37s - loss: 0.6830 - acc: 0.55 - ETA: 37s - loss: 0.6832 - acc: 0.55 - ETA: 36s - loss: 0.6835 - acc: 0.55 - ETA: 36s - loss: 0.6835 - acc: 0.55 - ETA: 35s - loss: 0.6831 - acc: 0.55 - ETA: 34s - loss: 0.6832 - acc: 0.55 - ETA: 34s - loss: 0.6835 - acc: 0.55 - ETA: 33s - loss: 0.6834 - acc: 0.55 - ETA: 33s - loss: 0.6827 - acc: 0.55 - ETA: 32s - loss: 0.6823 - acc: 0.55 - ETA: 31s - loss: 0.6821 - acc: 0.55 - ETA: 31s - loss: 0.6815 - acc: 0.56 - ETA: 30s - loss: 0.6803 - acc: 0.56 - ETA: 30s - loss: 0.6799 - acc: 0.56 - ETA: 29s - loss: 0.6788 - acc: 0.56 - ETA: 28s - loss: 0.6793 - acc: 0.56 - ETA: 28s - loss: 0.6788 - acc: 0.56 - ETA: 27s - loss: 0.6787 - acc: 0.56 - ETA: 27s - loss: 0.6788 - acc: 0.56 - ETA: 26s - loss: 0.6787 - acc: 0.56 - ETA: 25s - loss: 0.6800 - acc: 0.56 - ETA: 25s - loss: 0.6797 - acc: 0.56 - ETA: 24s - loss: 0.6798 - acc: 0.56 - ETA: 24s - loss: 0.6797 - acc: 0.56 - ETA: 23s - loss: 0.6789 - acc: 0.56 - ETA: 22s - loss: 0.6778 - acc: 0.56 - ETA: 22s - loss: 0.6782 - acc: 0.56 - ETA: 21s - loss: 0.6803 - acc: 0.56 - ETA: 21s - loss: 0.6795 - acc: 0.56 - ETA: 20s - loss: 0.6798 - acc: 0.56 - ETA: 19s - loss: 0.6792 - acc: 0.56 - ETA: 19s - loss: 0.6793 - acc: 0.56 - ETA: 18s - loss: 0.6787 - acc: 0.56 - ETA: 18s - loss: 0.6787 - acc: 0.57 - ETA: 17s - loss: 0.6792 - acc: 0.56 - ETA: 16s - loss: 0.6797 - acc: 0.56 - ETA: 16s - loss: 0.6796 - acc: 0.56 - ETA: 15s - loss: 0.6792 - acc: 0.56 - ETA: 14s - loss: 0.6790 - acc: 0.56 - ETA: 14s - loss: 0.6789 - acc: 0.56 - ETA: 13s - loss: 0.6787 - acc: 0.56 - ETA: 13s - loss: 0.6785 - acc: 0.56 - ETA: 12s - loss: 0.6784 - acc: 0.56 - ETA: 11s - loss: 0.6779 - acc: 0.56 - ETA: 11s - loss: 0.6771 - acc: 0.56 - ETA: 10s - loss: 0.6776 - acc: 0.56 - ETA: 10s - loss: 0.6775 - acc: 0.56 - ETA: 9s - loss: 0.6774 - acc: 0.5694 - ETA: 8s - loss: 0.6770 - acc: 0.570 - ETA: 8s - loss: 0.6780 - acc: 0.569 - ETA: 7s - loss: 0.6781 - acc: 0.568 - ETA: 7s - loss: 0.6777 - acc: 0.569 - ETA: 6s - loss: 0.6782 - acc: 0.569 - ETA: 5s - loss: 0.6789 - acc: 0.568 - ETA: 5s - loss: 0.6783 - acc: 0.569 - ETA: 4s - loss: 0.6782 - acc: 0.569 - ETA: 4s - loss: 0.6781 - acc: 0.569 - ETA: 3s - loss: 0.6783 - acc: 0.569 - ETA: 2s - loss: 0.6791 - acc: 0.568 - ETA: 2s - loss: 0.6790 - acc: 0.567 - ETA: 1s - loss: 0.6785 - acc: 0.568 - ETA: 1s - loss: 0.6794 - acc: 0.567 - ETA: 0s - loss: 0.6794 - acc: 0.566 - 88s 32ms/step - loss: 0.6794 - acc: 0.5664 - val_loss: 0.6783 - val_acc: 0.5756\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2793/2793 [==============================] - ETA: 1:22 - loss: 0.6453 - acc: 0.600 - ETA: 1:21 - loss: 0.5926 - acc: 0.725 - ETA: 1:20 - loss: 0.6280 - acc: 0.650 - ETA: 1:21 - loss: 0.6242 - acc: 0.675 - ETA: 1:20 - loss: 0.6329 - acc: 0.660 - ETA: 1:20 - loss: 0.6419 - acc: 0.633 - ETA: 1:19 - loss: 0.6264 - acc: 0.657 - ETA: 1:18 - loss: 0.6235 - acc: 0.662 - ETA: 1:18 - loss: 0.6180 - acc: 0.677 - ETA: 1:18 - loss: 0.6218 - acc: 0.660 - ETA: 1:17 - loss: 0.6158 - acc: 0.668 - ETA: 1:17 - loss: 0.6194 - acc: 0.675 - ETA: 1:16 - loss: 0.6199 - acc: 0.673 - ETA: 1:15 - loss: 0.6092 - acc: 0.682 - ETA: 1:15 - loss: 0.6068 - acc: 0.683 - ETA: 1:14 - loss: 0.5995 - acc: 0.693 - ETA: 1:13 - loss: 0.6125 - acc: 0.676 - ETA: 1:13 - loss: 0.6024 - acc: 0.683 - ETA: 1:12 - loss: 0.5949 - acc: 0.689 - ETA: 1:12 - loss: 0.5993 - acc: 0.690 - ETA: 1:11 - loss: 0.6096 - acc: 0.683 - ETA: 1:10 - loss: 0.6271 - acc: 0.670 - ETA: 1:10 - loss: 0.6392 - acc: 0.660 - ETA: 1:09 - loss: 0.6445 - acc: 0.656 - ETA: 1:08 - loss: 0.6479 - acc: 0.656 - ETA: 1:08 - loss: 0.6442 - acc: 0.659 - ETA: 1:07 - loss: 0.6452 - acc: 0.655 - ETA: 1:07 - loss: 0.6463 - acc: 0.657 - ETA: 1:06 - loss: 0.6454 - acc: 0.660 - ETA: 1:05 - loss: 0.6473 - acc: 0.655 - ETA: 1:05 - loss: 0.6478 - acc: 0.654 - ETA: 1:04 - loss: 0.6495 - acc: 0.650 - ETA: 1:04 - loss: 0.6519 - acc: 0.639 - ETA: 1:03 - loss: 0.6532 - acc: 0.633 - ETA: 1:02 - loss: 0.6543 - acc: 0.632 - ETA: 1:02 - loss: 0.6558 - acc: 0.625 - ETA: 1:01 - loss: 0.6564 - acc: 0.624 - ETA: 1:01 - loss: 0.6570 - acc: 0.623 - ETA: 1:00 - loss: 0.6577 - acc: 0.621 - ETA: 59s - loss: 0.6584 - acc: 0.620 - ETA: 59s - loss: 0.6607 - acc: 0.61 - ETA: 58s - loss: 0.6626 - acc: 0.61 - ETA: 58s - loss: 0.6632 - acc: 0.60 - ETA: 57s - loss: 0.6654 - acc: 0.60 - ETA: 57s - loss: 0.6665 - acc: 0.60 - ETA: 56s - loss: 0.6662 - acc: 0.60 - ETA: 56s - loss: 0.6664 - acc: 0.60 - ETA: 55s - loss: 0.6668 - acc: 0.60 - ETA: 55s - loss: 0.6677 - acc: 0.59 - ETA: 54s - loss: 0.6678 - acc: 0.60 - ETA: 54s - loss: 0.6686 - acc: 0.59 - ETA: 53s - loss: 0.6691 - acc: 0.59 - ETA: 52s - loss: 0.6694 - acc: 0.59 - ETA: 52s - loss: 0.6698 - acc: 0.59 - ETA: 51s - loss: 0.6702 - acc: 0.59 - ETA: 51s - loss: 0.6706 - acc: 0.58 - ETA: 50s - loss: 0.6711 - acc: 0.58 - ETA: 49s - loss: 0.6716 - acc: 0.58 - ETA: 49s - loss: 0.6717 - acc: 0.58 - ETA: 48s - loss: 0.6720 - acc: 0.58 - ETA: 48s - loss: 0.6723 - acc: 0.58 - ETA: 47s - loss: 0.6726 - acc: 0.58 - ETA: 46s - loss: 0.6727 - acc: 0.58 - ETA: 46s - loss: 0.6731 - acc: 0.58 - ETA: 45s - loss: 0.6729 - acc: 0.58 - ETA: 44s - loss: 0.6733 - acc: 0.58 - ETA: 44s - loss: 0.6732 - acc: 0.58 - ETA: 43s - loss: 0.6739 - acc: 0.58 - ETA: 43s - loss: 0.6744 - acc: 0.58 - ETA: 42s - loss: 0.6759 - acc: 0.58 - ETA: 41s - loss: 0.6771 - acc: 0.57 - ETA: 41s - loss: 0.6770 - acc: 0.57 - ETA: 40s - loss: 0.6771 - acc: 0.57 - ETA: 40s - loss: 0.6773 - acc: 0.57 - ETA: 39s - loss: 0.6775 - acc: 0.57 - ETA: 38s - loss: 0.6779 - acc: 0.57 - ETA: 38s - loss: 0.6781 - acc: 0.57 - ETA: 37s - loss: 0.6784 - acc: 0.57 - ETA: 37s - loss: 0.6783 - acc: 0.57 - ETA: 36s - loss: 0.6783 - acc: 0.57 - ETA: 35s - loss: 0.6790 - acc: 0.56 - ETA: 35s - loss: 0.6790 - acc: 0.56 - ETA: 34s - loss: 0.6792 - acc: 0.56 - ETA: 33s - loss: 0.6792 - acc: 0.56 - ETA: 33s - loss: 0.6799 - acc: 0.56 - ETA: 32s - loss: 0.6802 - acc: 0.56 - ETA: 32s - loss: 0.6799 - acc: 0.56 - ETA: 31s - loss: 0.6804 - acc: 0.56 - ETA: 30s - loss: 0.6804 - acc: 0.56 - ETA: 30s - loss: 0.6804 - acc: 0.56 - ETA: 29s - loss: 0.6806 - acc: 0.56 - ETA: 29s - loss: 0.6808 - acc: 0.56 - ETA: 28s - loss: 0.6808 - acc: 0.56 - ETA: 27s - loss: 0.6810 - acc: 0.56 - ETA: 27s - loss: 0.6812 - acc: 0.56 - ETA: 26s - loss: 0.6813 - acc: 0.55 - ETA: 25s - loss: 0.6815 - acc: 0.55 - ETA: 25s - loss: 0.6816 - acc: 0.55 - ETA: 24s - loss: 0.6817 - acc: 0.55 - ETA: 24s - loss: 0.6816 - acc: 0.55 - ETA: 23s - loss: 0.6819 - acc: 0.55 - ETA: 22s - loss: 0.6821 - acc: 0.55 - ETA: 22s - loss: 0.6819 - acc: 0.55 - ETA: 21s - loss: 0.6823 - acc: 0.55 - ETA: 21s - loss: 0.6822 - acc: 0.55 - ETA: 20s - loss: 0.6823 - acc: 0.55 - ETA: 19s - loss: 0.6825 - acc: 0.55 - ETA: 19s - loss: 0.6824 - acc: 0.55 - ETA: 18s - loss: 0.6824 - acc: 0.55 - ETA: 17s - loss: 0.6830 - acc: 0.55 - ETA: 17s - loss: 0.6831 - acc: 0.55 - ETA: 16s - loss: 0.6832 - acc: 0.55 - ETA: 16s - loss: 0.6834 - acc: 0.55 - ETA: 15s - loss: 0.6835 - acc: 0.55 - ETA: 14s - loss: 0.6835 - acc: 0.55 - ETA: 14s - loss: 0.6836 - acc: 0.55 - ETA: 13s - loss: 0.6838 - acc: 0.55 - ETA: 13s - loss: 0.6837 - acc: 0.55 - ETA: 12s - loss: 0.6840 - acc: 0.55 - ETA: 11s - loss: 0.6842 - acc: 0.55 - ETA: 11s - loss: 0.6846 - acc: 0.54 - ETA: 10s - loss: 0.6849 - acc: 0.54 - ETA: 10s - loss: 0.6847 - acc: 0.54 - ETA: 9s - loss: 0.6846 - acc: 0.5488 - ETA: 8s - loss: 0.6848 - acc: 0.548 - ETA: 8s - loss: 0.6848 - acc: 0.548 - ETA: 7s - loss: 0.6849 - acc: 0.549 - ETA: 7s - loss: 0.6849 - acc: 0.550 - ETA: 6s - loss: 0.6850 - acc: 0.550 - ETA: 5s - loss: 0.6852 - acc: 0.548 - ETA: 5s - loss: 0.6852 - acc: 0.548 - ETA: 4s - loss: 0.6854 - acc: 0.547 - ETA: 4s - loss: 0.6855 - acc: 0.547 - ETA: 3s - loss: 0.6857 - acc: 0.545 - ETA: 2s - loss: 0.6856 - acc: 0.546 - ETA: 2s - loss: 0.6858 - acc: 0.546 - ETA: 1s - loss: 0.6857 - acc: 0.547 - ETA: 0s - loss: 0.6858 - acc: 0.547 - ETA: 0s - loss: 0.6858 - acc: 0.547 - 88s 31ms/step - loss: 0.6859 - acc: 0.5467 - val_loss: 0.6843 - val_acc: 0.5627\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2793/2793 [==============================] - ETA: 1:23 - loss: 0.7178 - acc: 0.400 - ETA: 1:22 - loss: 0.7081 - acc: 0.450 - ETA: 1:21 - loss: 0.7001 - acc: 0.500 - ETA: 1:20 - loss: 0.7014 - acc: 0.487 - ETA: 1:20 - loss: 0.7036 - acc: 0.470 - ETA: 1:19 - loss: 0.6995 - acc: 0.491 - ETA: 1:18 - loss: 0.6975 - acc: 0.514 - ETA: 1:18 - loss: 0.6978 - acc: 0.500 - ETA: 1:17 - loss: 0.6965 - acc: 0.516 - ETA: 1:17 - loss: 0.6964 - acc: 0.515 - ETA: 1:16 - loss: 0.6955 - acc: 0.518 - ETA: 1:16 - loss: 0.6951 - acc: 0.525 - ETA: 1:15 - loss: 0.6942 - acc: 0.530 - ETA: 1:14 - loss: 0.6939 - acc: 0.528 - ETA: 1:14 - loss: 0.6930 - acc: 0.533 - ETA: 1:13 - loss: 0.6944 - acc: 0.525 - ETA: 1:12 - loss: 0.6939 - acc: 0.526 - ETA: 1:12 - loss: 0.6962 - acc: 0.511 - ETA: 1:11 - loss: 0.6959 - acc: 0.513 - ETA: 1:11 - loss: 0.6952 - acc: 0.515 - ETA: 1:11 - loss: 0.6950 - acc: 0.516 - ETA: 1:10 - loss: 0.6951 - acc: 0.511 - ETA: 1:09 - loss: 0.6951 - acc: 0.510 - ETA: 1:09 - loss: 0.6950 - acc: 0.510 - ETA: 1:08 - loss: 0.6952 - acc: 0.506 - ETA: 1:08 - loss: 0.6951 - acc: 0.509 - ETA: 1:07 - loss: 0.6946 - acc: 0.514 - ETA: 1:06 - loss: 0.6942 - acc: 0.516 - ETA: 1:06 - loss: 0.6948 - acc: 0.510 - ETA: 1:05 - loss: 0.6952 - acc: 0.508 - ETA: 1:04 - loss: 0.6953 - acc: 0.506 - ETA: 1:04 - loss: 0.6951 - acc: 0.507 - ETA: 1:03 - loss: 0.6947 - acc: 0.513 - ETA: 1:03 - loss: 0.6945 - acc: 0.513 - ETA: 1:02 - loss: 0.6943 - acc: 0.512 - ETA: 1:01 - loss: 0.6938 - acc: 0.513 - ETA: 1:01 - loss: 0.6936 - acc: 0.514 - ETA: 1:00 - loss: 0.6938 - acc: 0.511 - ETA: 1:00 - loss: 0.6948 - acc: 0.505 - ETA: 59s - loss: 0.6948 - acc: 0.501 - ETA: 58s - loss: 0.6944 - acc: 0.50 - ETA: 58s - loss: 0.6947 - acc: 0.50 - ETA: 57s - loss: 0.6950 - acc: 0.50 - ETA: 57s - loss: 0.6952 - acc: 0.49 - ETA: 56s - loss: 0.6953 - acc: 0.49 - ETA: 55s - loss: 0.6951 - acc: 0.49 - ETA: 55s - loss: 0.6949 - acc: 0.49 - ETA: 54s - loss: 0.6949 - acc: 0.50 - ETA: 54s - loss: 0.6959 - acc: 0.49 - ETA: 53s - loss: 0.6958 - acc: 0.49 - ETA: 52s - loss: 0.6962 - acc: 0.49 - ETA: 52s - loss: 0.6962 - acc: 0.49 - ETA: 51s - loss: 0.6961 - acc: 0.49 - ETA: 51s - loss: 0.6957 - acc: 0.50 - ETA: 50s - loss: 0.6958 - acc: 0.49 - ETA: 50s - loss: 0.6956 - acc: 0.50 - ETA: 49s - loss: 0.6956 - acc: 0.50 - ETA: 48s - loss: 0.6958 - acc: 0.49 - ETA: 48s - loss: 0.6959 - acc: 0.49 - ETA: 47s - loss: 0.6956 - acc: 0.49 - ETA: 47s - loss: 0.6957 - acc: 0.49 - ETA: 46s - loss: 0.6956 - acc: 0.49 - ETA: 45s - loss: 0.6955 - acc: 0.49 - ETA: 45s - loss: 0.6954 - acc: 0.50 - ETA: 44s - loss: 0.6955 - acc: 0.49 - ETA: 44s - loss: 0.6954 - acc: 0.49 - ETA: 43s - loss: 0.6954 - acc: 0.49 - ETA: 42s - loss: 0.6953 - acc: 0.49 - ETA: 42s - loss: 0.6953 - acc: 0.49 - ETA: 41s - loss: 0.6951 - acc: 0.50 - ETA: 41s - loss: 0.6951 - acc: 0.50 - ETA: 40s - loss: 0.6951 - acc: 0.50 - ETA: 39s - loss: 0.6950 - acc: 0.49 - ETA: 39s - loss: 0.6952 - acc: 0.49 - ETA: 38s - loss: 0.6954 - acc: 0.49 - ETA: 38s - loss: 0.6953 - acc: 0.49 - ETA: 37s - loss: 0.6955 - acc: 0.49 - ETA: 36s - loss: 0.6954 - acc: 0.49 - ETA: 36s - loss: 0.6952 - acc: 0.49 - ETA: 35s - loss: 0.6951 - acc: 0.50 - ETA: 35s - loss: 0.6951 - acc: 0.50 - ETA: 34s - loss: 0.6950 - acc: 0.50 - ETA: 33s - loss: 0.6950 - acc: 0.50 - ETA: 33s - loss: 0.6947 - acc: 0.50 - ETA: 32s - loss: 0.6947 - acc: 0.50 - ETA: 32s - loss: 0.6947 - acc: 0.50 - ETA: 31s - loss: 0.6946 - acc: 0.50 - ETA: 30s - loss: 0.6944 - acc: 0.50 - ETA: 30s - loss: 0.6947 - acc: 0.50 - ETA: 29s - loss: 0.6945 - acc: 0.50 - ETA: 29s - loss: 0.6943 - acc: 0.50 - ETA: 28s - loss: 0.6944 - acc: 0.50 - ETA: 27s - loss: 0.6942 - acc: 0.50 - ETA: 27s - loss: 0.6941 - acc: 0.50 - ETA: 26s - loss: 0.6939 - acc: 0.50 - ETA: 26s - loss: 0.6938 - acc: 0.50 - ETA: 25s - loss: 0.6928 - acc: 0.51 - ETA: 24s - loss: 0.6932 - acc: 0.50 - ETA: 24s - loss: 0.6940 - acc: 0.50 - ETA: 23s - loss: 0.6939 - acc: 0.50 - ETA: 23s - loss: 0.6939 - acc: 0.50 - ETA: 22s - loss: 0.6933 - acc: 0.50 - ETA: 21s - loss: 0.6931 - acc: 0.50 - ETA: 21s - loss: 0.6935 - acc: 0.50 - ETA: 20s - loss: 0.6937 - acc: 0.51 - ETA: 20s - loss: 0.6942 - acc: 0.50 - ETA: 19s - loss: 0.6942 - acc: 0.50 - ETA: 18s - loss: 0.6942 - acc: 0.50 - ETA: 18s - loss: 0.6950 - acc: 0.50 - ETA: 17s - loss: 0.6950 - acc: 0.50 - ETA: 17s - loss: 0.6949 - acc: 0.50 - ETA: 16s - loss: 0.6950 - acc: 0.50 - ETA: 15s - loss: 0.6948 - acc: 0.50 - ETA: 15s - loss: 0.6951 - acc: 0.50 - ETA: 14s - loss: 0.6949 - acc: 0.51 - ETA: 14s - loss: 0.6954 - acc: 0.50 - ETA: 13s - loss: 0.6957 - acc: 0.50 - ETA: 12s - loss: 0.6963 - acc: 0.50 - ETA: 12s - loss: 0.6963 - acc: 0.50 - ETA: 11s - loss: 0.6962 - acc: 0.50 - ETA: 11s - loss: 0.6958 - acc: 0.50 - ETA: 10s - loss: 0.6957 - acc: 0.50 - ETA: 9s - loss: 0.6953 - acc: 0.5081 - ETA: 9s - loss: 0.6949 - acc: 0.508 - ETA: 8s - loss: 0.6957 - acc: 0.508 - ETA: 8s - loss: 0.6968 - acc: 0.507 - ETA: 7s - loss: 0.6980 - acc: 0.505 - ETA: 6s - loss: 0.6988 - acc: 0.505 - ETA: 6s - loss: 0.6993 - acc: 0.504 - ETA: 5s - loss: 0.6991 - acc: 0.505 - ETA: 5s - loss: 0.6992 - acc: 0.503 - ETA: 4s - loss: 0.6992 - acc: 0.503 - ETA: 3s - loss: 0.6992 - acc: 0.503 - ETA: 3s - loss: 0.6992 - acc: 0.502 - ETA: 2s - loss: 0.6993 - acc: 0.502 - ETA: 2s - loss: 0.6994 - acc: 0.502 - ETA: 1s - loss: 0.6994 - acc: 0.502 - ETA: 0s - loss: 0.6997 - acc: 0.501 - ETA: 0s - loss: 0.6998 - acc: 0.501 - 87s 31ms/step - loss: 0.6997 - acc: 0.5023 - val_loss: 0.6891 - val_acc: 0.5659\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x240882c4f98>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Train the model \n",
    "model.fit(pretrain_X, pretrain_y, batch_size=20, epochs=4,verbose=1,validation_data=(val_X, val_y))\n",
    "model.fit(train_X, train_y, batch_size=20, epochs=4,verbose=1,validation_data=(val_X, val_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "6a1ae9ca76b82adb6c88824c35b9ccabb4bec612"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311/311 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 3s 11ms/step\n",
      "[0.6891399209905668, 0.5659163967972783]\n",
      "Validation Loss: 0.6891399209905668\n",
      " Validation Accuracy: 0.5659163967972783\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pred_val_y = model.predict([val_X], batch_size=20, verbose=1)\n",
    "\n",
    "valid_score = model.evaluate(val_X, val_y, batch_size=20)\n",
    "\n",
    "print(valid_score)\n",
    "print('Validation Loss: {}\\n Validation Accuracy: {}\\n'.format(valid_score[0], valid_score[1]))\n",
    "\n",
    "# thresholds = []\n",
    "# for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "#     thresh = np.round(thresh, 2)\n",
    "#     res = metrics.f1_score(val_y, (pred_val_y > thresh).astype(int))\n",
    "#     thresholds.append([thresh, res])\n",
    "#     print(\"F1 score at threshold {0} is {1}\".format(thresh, res))\n",
    "    \n",
    "# thresholds.sort(key=lambda x: x[1], reverse=True)\n",
    "# best_thresh = thresholds[0][0]\n",
    "# print(\"Best threshold: \", best_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6891399209905668, 0.5659163967972783]\n"
     ]
    }
   ],
   "source": [
    "print(valid_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3130e796074bd8ecf6d35a6121593b4af5667f8e"
   },
   "outputs": [],
   "source": [
    "pred_test_y = model.predict([test_X], batch_size=1024, verbose=1)\n",
    "pred_test_y = pred_test_y.argmax(axis=-1) + 1\n",
    "out_df = pd.DataFrame({\"review_id\":test_df[\"review_id\"].values})\n",
    "out_df['pred'] = pred_test_y\n",
    "out_df.to_csv(\"pred.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
